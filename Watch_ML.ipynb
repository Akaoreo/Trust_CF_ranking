{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMcgHq1-rUJ3",
        "outputId": "9c120a48-a49d-4951-b586-2e7e3df215de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85xdyb3WSKd2",
        "outputId": "9bb2d169-ac99-481a-d071-7247f7418ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMRanker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "# Convert scores to 1-100 scale and discretize into 10 levels\n",
        "df['reason_score_100'] = df['reason_score'] * 10  # Convert to 1-100 scale\n",
        "df['reason_score_discrete'] = pd.qcut(df['reason_score_100'], q=10, labels=False, duplicates='drop')\n",
        "\n",
        "# Enhanced feature engineering focusing on relative comparisons\n",
        "def engineer_features(df):\n",
        "    # Basic numeric conversion\n",
        "    df['Impact_Factor'] = pd.to_numeric(df['Impact_Factor'], errors='coerce')\n",
        "    df['original_altmetric_score'] = pd.to_numeric(df['original_altmetric_score'], errors='coerce')\n",
        "    df['original_cited_by_posts_count'] = pd.to_numeric(df['original_cited_by_posts_count'], errors='coerce')\n",
        "\n",
        "    # Fill NaN with 0\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    # Create percentile ranks for each metric\n",
        "    df['impact_factor_rank'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['altmetric_rank'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['citation_rank'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Create interaction features based on ranks\n",
        "    df['impact_citation_rank'] = df['impact_factor_rank'] * df['citation_rank']\n",
        "    df['impact_altmetric_rank'] = df['impact_factor_rank'] * df['altmetric_rank']\n",
        "\n",
        "    # Relative position features\n",
        "    df['relative_citation_impact'] = df['original_cited_by_posts_count'] / (df['Impact_Factor'] + 1)\n",
        "    df['relative_altmetric_impact'] = df['original_altmetric_score'] / (df['Impact_Factor'] + 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "# Define features focusing on relative positions\n",
        "features = [\n",
        "    'impact_factor_rank',\n",
        "    'altmetric_rank',\n",
        "    'citation_rank',\n",
        "    'impact_citation_rank',\n",
        "    'impact_altmetric_rank',\n",
        "    'relative_citation_impact',\n",
        "    'relative_altmetric_impact',\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count'\n",
        "]\n",
        "\n",
        "# Prepare data\n",
        "X = df_engineered[features].copy()\n",
        "y = df['reason_score_discrete']  # Using discretized scores\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Create query groups based on discrete scores\n",
        "def create_query_groups(y):\n",
        "    return np.bincount(y.astype(int))[np.nonzero(np.bincount(y.astype(int)))]\n",
        "\n",
        "train_query_groups = create_query_groups(y_train)\n",
        "test_query_groups = create_query_groups(y_test)\n",
        "\n",
        "# Initialize model with ranking-specific parameters\n",
        "ranker = LGBMRanker(\n",
        "    objective=\"lambdarank\",\n",
        "    metric=\"ndcg\",\n",
        "    boosting_type=\"gbdt\",\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    num_leaves=31,\n",
        "    importance_type=\"gain\",\n",
        "    random_state=42,\n",
        "    label_gain=list(range(10))  # 10 levels (0-9)\n",
        ")\n",
        "\n",
        "# Train model\n",
        "ranker.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    group=train_query_groups,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_group=[test_query_groups],\n",
        "    eval_metric='ndcg'\n",
        ")\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': features,\n",
        "    'importance': ranker.feature_importances_\n",
        "})\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance.sort_values('importance', ascending=False))\n",
        "\n",
        "def analyze_sample_rankings(X_test, y_test, df_original, model, n_samples=10):\n",
        "    # Get predictions for all test data\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Randomly sample n_samples entries\n",
        "    random_indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
        "\n",
        "    # Get the sampled data\n",
        "    X_sample = X_test.iloc[random_indices]\n",
        "    y_sample = y_test.iloc[random_indices]\n",
        "\n",
        "    # Get predictions for just these samples\n",
        "    sample_predictions = model.predict(X_sample)\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results = pd.DataFrame({\n",
        "        'Title': df_original['Title'].iloc[y_sample.index],\n",
        "        'Predicted_Score': sample_predictions,\n",
        "        'Actual_Score': df_original['reason_score'].iloc[y_sample.index],\n",
        "        'Original_Features': [\n",
        "            f\"IF:{X_sample['Impact_Factor'].iloc[i]:.2f}, \"\n",
        "            f\"Alt:{X_sample['original_altmetric_score'].iloc[i]:.2f}, \"\n",
        "            f\"Cit:{X_sample['original_cited_by_posts_count'].iloc[i]}\"\n",
        "            for i in range(len(X_sample))\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Add rankings (1-10) for just these samples\n",
        "    results['Predicted_Rank'] = results['Predicted_Score'].rank(ascending=False)\n",
        "    results['True_Rank'] = results['Actual_Score'].rank(ascending=False)\n",
        "\n",
        "    # Sort by true rank to show ground truth order\n",
        "    results = results.sort_values('True_Rank')\n",
        "\n",
        "    print(\"\\nComparison of Rankings for 10 Random Papers:\")\n",
        "    print(\"=\" * 100)\n",
        "    print(\"\\nOrdered by Ground Truth Rank:\")\n",
        "    for idx, row in results.iterrows():\n",
        "        print(f\"\\nTrue Rank: {int(row['True_Rank'])} | Predicted Rank: {int(row['Predicted_Rank'])}\")\n",
        "        print(f\"Title: {row['Title']}\")\n",
        "        print(f\"True Score: {row['Actual_Score']:.2f} | Predicted Score: {row['Predicted_Score']:.2f}\")\n",
        "        print(f\"Features: {row['Original_Features']}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Calculate rank correlation for just these 10 samples\n",
        "    rank_correlation = results['Predicted_Rank'].corr(results['True_Rank'], method='spearman')\n",
        "    print(f\"\\nSpearman Rank Correlation for these 10 papers: {rank_correlation:.3f}\")\n",
        "\n",
        "    # Calculate perfect matches and close matches\n",
        "    perfect_matches = (results['Predicted_Rank'] == results['True_Rank']).sum()\n",
        "    close_matches = (abs(results['Predicted_Rank'] - results['True_Rank']) <= 1).sum()\n",
        "\n",
        "    print(f\"Perfect Rank Matches: {perfect_matches} out of 10\")\n",
        "    print(f\"Ranks Off by ≤1 Position: {close_matches} out of 10\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the analysis\n",
        "sample_results = analyze_sample_rankings(X_test, y_test, df, ranker)\n",
        "\n",
        "# Calculate NDCG@k for different k values\n",
        "predictions_all = ranker.predict(X_test)\n",
        "for k in [5, 10, 20]:\n",
        "    ndcg = ndcg_score(\n",
        "        y_true=y_test.values.reshape(1, -1),\n",
        "        y_score=predictions_all.reshape(1, -1),\n",
        "        k=k\n",
        "    )\n",
        "    print(f\"\\nNDCG@{k}: {ndcg:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "import pickle\n",
        "\n",
        "model_data = {\n",
        "    'ranker': ranker,\n",
        "    'feature_scaler': StandardScaler(),\n",
        "    'features': features\n",
        "}\n",
        "\n",
        "with open('ranking_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model_data, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JxnFGgiT6pP",
        "outputId": "ac02cfa0-ce2a-4acf-9f88-889103477d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002034 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2341\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 10\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "Feature Importance:\n",
            "                         feature  importance\n",
            "2                  citation_rank  254.776072\n",
            "5       relative_citation_impact  227.883722\n",
            "0             impact_factor_rank  186.029466\n",
            "4          impact_altmetric_rank  143.267823\n",
            "6      relative_altmetric_impact  140.065374\n",
            "1                 altmetric_rank  102.489540\n",
            "3           impact_citation_rank   78.189598\n",
            "7                  Impact_Factor    0.000000\n",
            "8       original_altmetric_score    0.000000\n",
            "9  original_cited_by_posts_count    0.000000\n",
            "\n",
            "Comparison of Rankings for 10 Random Papers:\n",
            "====================================================================================================\n",
            "\n",
            "Ordered by Ground Truth Rank:\n",
            "\n",
            "True Rank: 1 | Predicted Rank: 2\n",
            "Title: Down-regulation of XIAP enhances the radiosensitivity of esophageal cancer cells in vivo and in vitro\n",
            "True Score: 10.00 | Predicted Score: -1.06\n",
            "Features: IF:3.80, Alt:8.08, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 1 | Predicted Rank: 7\n",
            "Title: Triple signalling mode carbon dots-based biodegradable molecularly imprinted polymer as a multi-tasking visual sensor for rapid and “on-site” monitoring of silver ions\n",
            "True Score: 10.00 | Predicted Score: -1.29\n",
            "Features: IF:5.70, Alt:26.33, Cit:4.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted Rank: 8\n",
            "Title: Personality, Stress and Disease: Description and Validation of a New Inventory\n",
            "True Score: 8.60 | Predicted Score: -1.50\n",
            "Features: IF:1.70, Alt:22.50, Cit:4.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted Rank: 3\n",
            "Title: Doctors in Star Trek: Dr. Phlox in Star Trek: Enterprise\n",
            "True Score: 8.30 | Predicted Score: -1.14\n",
            "Features: IF:2.20, Alt:4.00, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 5 | Predicted Rank: 1\n",
            "Title: Performance of the Simplified American Academy of Pediatrics Table to Screen Elevated Blood Pressure in Children\n",
            "True Score: 6.70 | Predicted Score: -0.67\n",
            "Features: IF:24.70, Alt:12.80, Cit:36.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted Rank: 4\n",
            "Title: Megasphaera vaginalis sp. nov. and Anaerococcus vaginimassiliensis sp. nov., new bacteria isolated from vagina of French woman with bacterial vaginosis\n",
            "True Score: 6.30 | Predicted Score: -1.18\n",
            "Features: IF:2.90, Alt:6.85, Cit:9.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted Rank: 5\n",
            "Title: USP26 regulates TGF-beta signaling by deubiquitinating and stabilizing SMAD7\n",
            "True Score: 5.00 | Predicted Score: -1.19\n",
            "Features: IF:6.50, Alt:1.00, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted Rank: 10\n",
            "Title: X-ray spectroscopy: Transferring electrons to water\n",
            "True Score: 4.00 | Predicted Score: -1.69\n",
            "Features: IF:19.20, Alt:5.04, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted Rank: 9\n",
            "Title: Geochemistry of pelitic rocks from the Middle Permian Lucaogou Formation, Sangonghe area, Junggar Basin, Northwest China: implications for source weathering, recycling, provenance and tectonic setting\n",
            "True Score: 4.00 | Predicted Score: -1.55\n",
            "Features: IF:1.40, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted Rank: 6\n",
            "Title: Emergency medicine: diagnosis and management\n",
            "True Score: 4.00 | Predicted Score: -1.24\n",
            "Features: IF:4.20, Alt:0.50, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Rank Correlation for these 10 papers: 0.443\n",
            "Perfect Rank Matches: 1 out of 10\n",
            "Ranks Off by ≤1 Position: 4 out of 10\n",
            "\n",
            "NDCG@5: 0.8141\n",
            "\n",
            "NDCG@10: 0.8490\n",
            "\n",
            "NDCG@20: 0.8589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMRanker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "# Convert scores to 1-100 scale with more granularity\n",
        "df['reason_score_100'] = (df['reason_score'] * 10).clip(1, 100)  # Ensure scores are between 1-100\n",
        "# Create 20 discrete levels for better granularity\n",
        "df['reason_score_discrete'] = pd.qcut(df['reason_score_100'], q=20, labels=False, duplicates='drop')\n",
        "\n",
        "def engineer_features(df):\n",
        "    # Basic numeric conversion\n",
        "    df['Impact_Factor'] = pd.to_numeric(df['Impact_Factor'], errors='coerce')\n",
        "    df['original_altmetric_score'] = pd.to_numeric(df['original_altmetric_score'], errors='coerce')\n",
        "    df['original_cited_by_posts_count'] = pd.to_numeric(df['original_cited_by_posts_count'], errors='coerce')\n",
        "\n",
        "    # Fill NaN with 0\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    # Create percentile ranks for each metric\n",
        "    df['impact_factor_rank'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['altmetric_rank'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['citation_rank'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Create interaction features based on ranks\n",
        "    df['impact_citation_rank'] = df['impact_factor_rank'] * df['citation_rank']\n",
        "    df['impact_altmetric_rank'] = df['impact_factor_rank'] * df['altmetric_rank']\n",
        "\n",
        "    # Relative position features with normalized scores\n",
        "    df['relative_citation_impact'] = df['original_cited_by_posts_count'] / (df['Impact_Factor'] + 1)\n",
        "    df['relative_altmetric_impact'] = df['original_altmetric_score'] / (df['Impact_Factor'] + 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "# Define features\n",
        "features = [\n",
        "    'impact_factor_rank',\n",
        "    'altmetric_rank',\n",
        "    'citation_rank',\n",
        "    'impact_citation_rank',\n",
        "    'impact_altmetric_rank',\n",
        "    'relative_citation_impact',\n",
        "    'relative_altmetric_impact',\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count'\n",
        "]\n",
        "\n",
        "# Prepare data\n",
        "X = df_engineered[features].copy()\n",
        "y = df['reason_score_discrete']  # Using 20-level discretized scores\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Create query groups based on discrete scores\n",
        "def create_query_groups(y):\n",
        "    return np.bincount(y.astype(int))[np.nonzero(np.bincount(y.astype(int)))]\n",
        "\n",
        "train_query_groups = create_query_groups(y_train)\n",
        "test_query_groups = create_query_groups(y_test)\n",
        "\n",
        "# Initialize model\n",
        "ranker = LGBMRanker(\n",
        "    objective=\"lambdarank\",\n",
        "    metric=\"ndcg\",\n",
        "    boosting_type=\"gbdt\",\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    num_leaves=31,\n",
        "    importance_type=\"gain\",\n",
        "    random_state=42,\n",
        "    label_gain=list(range(20))  # 20 levels for more granularity\n",
        ")\n",
        "\n",
        "# Train model\n",
        "ranker.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    group=train_query_groups,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_group=[test_query_groups],\n",
        "    eval_metric='ndcg'\n",
        ")\n",
        "\n",
        "def analyze_sample_rankings(X_test, y_test, df_original, model, n_samples=10):\n",
        "    # Get predictions for all test data\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Randomly sample n_samples entries\n",
        "    random_indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
        "\n",
        "    # Get the sampled data\n",
        "    X_sample = X_test.iloc[random_indices]\n",
        "    y_sample = y_test.iloc[random_indices]\n",
        "\n",
        "    # Get predictions for just these samples\n",
        "    sample_predictions = model.predict(X_sample)\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results = pd.DataFrame({\n",
        "        'Title': df_original['Title'].iloc[y_sample.index],\n",
        "        'Predicted_Score': sample_predictions,\n",
        "        'Actual_Score': df_original['reason_score_100'].iloc[y_sample.index],  # Using 100-scale scores\n",
        "        'Original_Features': [\n",
        "            f\"IF:{X_sample['Impact_Factor'].iloc[i]:.2f}, \"\n",
        "            f\"Alt:{X_sample['original_altmetric_score'].iloc[i]:.2f}, \"\n",
        "            f\"Cit:{X_sample['original_cited_by_posts_count'].iloc[i]}\"\n",
        "            for i in range(len(X_sample))\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Add rankings (1-10) for just these samples\n",
        "    results['Predicted_Rank'] = results['Predicted_Score'].rank(ascending=False)\n",
        "    results['True_Rank'] = results['Actual_Score'].rank(ascending=False)\n",
        "\n",
        "    # Sort by true rank to show ground truth order\n",
        "    results = results.sort_values('True_Rank')\n",
        "\n",
        "    print(\"\\nComparison of Rankings for 10 Random Papers:\")\n",
        "    print(\"=\" * 100)\n",
        "    print(\"\\nOrdered by Ground Truth Rank:\")\n",
        "    for idx, row in results.iterrows():\n",
        "        print(f\"\\nTrue Rank: {int(row['True_Rank'])} | Predicted Rank: {int(row['Predicted_Rank'])}\")\n",
        "        print(f\"Title: {row['Title']}\")\n",
        "        print(f\"True Score: {row['Actual_Score']:.1f} | Predicted Score: {row['Predicted_Score']:.2f}\")\n",
        "        print(f\"Features: {row['Original_Features']}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Calculate rank correlation for just these 10 samples\n",
        "    rank_correlation = results['Predicted_Rank'].corr(results['True_Rank'], method='spearman')\n",
        "    print(f\"\\nSpearman Rank Correlation for these 10 papers: {rank_correlation:.3f}\")\n",
        "\n",
        "    # Calculate perfect matches and close matches\n",
        "    perfect_matches = (results['Predicted_Rank'] == results['True_Rank']).sum()\n",
        "    close_matches = (abs(results['Predicted_Rank'] - results['True_Rank']) <= 1).sum()\n",
        "\n",
        "    print(f\"Perfect Rank Matches: {perfect_matches} out of 10\")\n",
        "    print(f\"Ranks Off by ≤1 Position: {close_matches} out of 10\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the analysis\n",
        "sample_results = analyze_sample_rankings(X_test, y_test, df, ranker)\n",
        "\n",
        "# Calculate NDCG@k for different k values\n",
        "predictions_all = ranker.predict(X_test)\n",
        "for k in [5, 10, 20]:\n",
        "    ndcg = ndcg_score(\n",
        "        y_true=y_test.values.reshape(1, -1),\n",
        "        y_score=predictions_all.reshape(1, -1),\n",
        "        k=k\n",
        "    )\n",
        "    print(f\"\\nNDCG@{k}: {ndcg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKACM5EvjdEA",
        "outputId": "9159338a-2a95-4ec2-f533-ccb66a9e34b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004210 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2343\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 10\n",
            "\n",
            "Comparison of Rankings for 10 Random Papers:\n",
            "====================================================================================================\n",
            "\n",
            "Ordered by Ground Truth Rank:\n",
            "\n",
            "True Rank: 2 | Predicted Rank: 5\n",
            "Title: Meta analysis of clinical prognosis of radiofrequency ablation versus partial nephrectomy in the treatment of early renal cell carcinoma\n",
            "True Score: 100.0 | Predicted Score: -1.26\n",
            "Features: IF:3.50, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted Rank: 7\n",
            "Title: Simple Sequence Repeat and S-locus Genotyping to Explore Genetic Variability in Polyploid Prunus spinosa and P. insititia\n",
            "True Score: 100.0 | Predicted Score: -1.62\n",
            "Features: IF:2.10, Alt:5.04, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted Rank: 8\n",
            "Title: NPTX1 Regulates Neural Lineage Specification from Human Pluripotent Stem Cells\n",
            "True Score: 100.0 | Predicted Score: -1.91\n",
            "Features: IF:7.50, Alt:16.78, Cit:13.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted Rank: 3\n",
            "Title: MicroRNA‑122 affects cell aggressiveness and apoptosis by targeting PKM2 in human hepatocellular carcinoma\n",
            "True Score: 100.0 | Predicted Score: -1.25\n",
            "Features: IF:3.80, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 5 | Predicted Rank: 6\n",
            "Title: Are European birds leaving traditional wintering grounds in the Mediterranean?\n",
            "True Score: 98.0 | Predicted Score: -1.62\n",
            "Features: IF:1.50, Alt:49.34, Cit:86.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted Rank: 2\n",
            "Title: Fabrication of Tantalum and Nitrogen Codoped ZnO (Ta, N-ZnO) Thin Films Using the Electrospay: Twin Applications as an Excellent Transparent Electrode and a Field Emitter\n",
            "True Score: 88.0 | Predicted Score: -0.81\n",
            "Features: IF:8.30, Alt:5.04, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted Rank: 10\n",
            "Title: Oligosaccharide Ligands for NKR-P1 Protein Activate NK Cells and Cytotoxicity\n",
            "True Score: 82.0 | Predicted Score: -2.04\n",
            "Features: IF:50.50, Alt:3.50, Cit:4.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted Rank: 4\n",
            "Title: EEG signal connectivity for characterizing interictal activity in patients with mesial temporal lobe epilepsy\n",
            "True Score: 65.0 | Predicted Score: -1.26\n",
            "Features: IF:2.70, Alt:2.25, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted Rank: 1\n",
            "Title: Selenium supplementation lowers insulin resistance and markers of cardio-metabolic risk in patients with congestive heart failure: a randomised, double-blind, placebo-controlled trial\n",
            "True Score: 58.0 | Predicted Score: 1.28\n",
            "Features: IF:3.00, Alt:8.25, Cit:8.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted Rank: 9\n",
            "Title: Mental health in UK Biobank: development, implementation and results from an online questionnaire completed by 157 366 participants\n",
            "True Score: 27.0 | Predicted Score: -2.01\n",
            "Features: IF:3.90, Alt:31.65, Cit:71.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Rank Correlation for these 10 papers: -0.025\n",
            "Perfect Rank Matches: 0 out of 10\n",
            "Ranks Off by ≤1 Position: 3 out of 10\n",
            "\n",
            "NDCG@5: 0.8931\n",
            "\n",
            "NDCG@10: 0.9306\n",
            "\n",
            "NDCG@20: 0.8774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMRanker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "# Convert scores to 1-100 scale\n",
        "df['reason_score_100'] = (df['reason_score'] * 10).clip(1, 100)\n",
        "\n",
        "# Enhanced feature engineering\n",
        "def engineer_features(df):\n",
        "    # Basic numeric conversion\n",
        "    df['Impact_Factor'] = pd.to_numeric(df['Impact_Factor'], errors='coerce')\n",
        "    df['original_altmetric_score'] = pd.to_numeric(df['original_altmetric_score'], errors='coerce')\n",
        "    df['original_cited_by_posts_count'] = pd.to_numeric(df['original_cited_by_posts_count'], errors='coerce')\n",
        "\n",
        "    # Fill NaN with 0\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    # Create percentile ranks\n",
        "    df['impact_factor_rank'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['altmetric_rank'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['citation_rank'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Severity-weighted features\n",
        "    severity_weight = df['reason_score_100'] / 100\n",
        "    df['severity_weighted_impact'] = df['Impact_Factor'] * severity_weight\n",
        "    df['severity_weighted_altmetric'] = df['original_altmetric_score'] * severity_weight\n",
        "    df['severity_weighted_citation'] = df['original_cited_by_posts_count'] * severity_weight\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "# Define features\n",
        "features = [\n",
        "    'impact_factor_rank',\n",
        "    'altmetric_rank',\n",
        "    'citation_rank',\n",
        "    'severity_weighted_impact',\n",
        "    'severity_weighted_altmetric',\n",
        "    'severity_weighted_citation',\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count'\n",
        "]\n",
        "\n",
        "# Prepare data\n",
        "X = df_engineered[features].copy()\n",
        "\n",
        "# Create severity groups (8 groups)\n",
        "y_groups = pd.qcut(df['reason_score_100'], q=8, labels=False, duplicates='drop')\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_groups, test_size=0.2, random_state=42, stratify=y_groups\n",
        ")\n",
        "\n",
        "# Create query groups\n",
        "def create_query_groups(y):\n",
        "    groups = []\n",
        "    for score in range(8):  # 8 severity groups\n",
        "        count = np.sum(y == score)\n",
        "        if count > 0:\n",
        "            groups.append(count)\n",
        "    return np.array(groups)\n",
        "\n",
        "train_query_groups = create_query_groups(y_train)\n",
        "test_query_groups = create_query_groups(y_test)\n",
        "\n",
        "# Initialize model\n",
        "ranker = LGBMRanker(\n",
        "    objective=\"lambdarank\",\n",
        "    metric=\"ndcg\",\n",
        "    boosting_type=\"gbdt\",\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=8,\n",
        "    num_leaves=31,\n",
        "    importance_type=\"gain\",\n",
        "    random_state=42,\n",
        "    label_gain=list(range(8))  # 8 severity groups\n",
        ")\n",
        "\n",
        "# Train model\n",
        "ranker.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    group=train_query_groups,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_group=[test_query_groups],\n",
        "    eval_metric='ndcg'\n",
        ")\n",
        "\n",
        "def analyze_sample_rankings(X_test, y_test, df_original, model, n_samples=10):\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Randomly sample n_samples entries\n",
        "    random_indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
        "\n",
        "    X_sample = X_test.iloc[random_indices]\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results = pd.DataFrame({\n",
        "        'Title': df_original['Title'].iloc[X_sample.index],\n",
        "        'Predicted_Score': model.predict(X_sample),\n",
        "        'Actual_Score': df_original['reason_score_100'].iloc[X_sample.index],\n",
        "        'Original_Features': [\n",
        "            f\"IF:{X_sample['Impact_Factor'].iloc[i]:.2f}, \"\n",
        "            f\"Alt:{X_sample['original_altmetric_score'].iloc[i]:.2f}, \"\n",
        "            f\"Cit:{X_sample['original_cited_by_posts_count'].iloc[i]}\"\n",
        "            for i in range(len(X_sample))\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Add rankings\n",
        "    results['Predicted_Rank'] = results['Predicted_Score'].rank(ascending=False)\n",
        "    results['True_Rank'] = results['Actual_Score'].rank(ascending=False)\n",
        "\n",
        "    # Sort by true rank\n",
        "    results = results.sort_values('True_Rank')\n",
        "\n",
        "    print(\"\\nComparison of Rankings for 10 Random Papers:\")\n",
        "    print(\"=\" * 100)\n",
        "    for idx, row in results.iterrows():\n",
        "        print(f\"\\nTrue Rank: {int(row['True_Rank'])} | Predicted Rank: {int(row['Predicted_Rank'])}\")\n",
        "        print(f\"Title: {row['Title']}\")\n",
        "        print(f\"True Score: {row['Actual_Score']:.1f} | Predicted Score: {row['Predicted_Score']:.2f}\")\n",
        "        print(f\"Features: {row['Original_Features']}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Calculate metrics\n",
        "    rank_correlation = results['Predicted_Rank'].corr(results['True_Rank'], method='spearman')\n",
        "    perfect_matches = (results['Predicted_Rank'] == results['True_Rank']).sum()\n",
        "    close_matches = (abs(results['Predicted_Rank'] - results['True_Rank']) <= 1).sum()\n",
        "\n",
        "    print(f\"\\nSpearman Rank Correlation for these 10 papers: {rank_correlation:.3f}\")\n",
        "    print(f\"Perfect Rank Matches: {perfect_matches} out of 10\")\n",
        "    print(f\"Ranks Off by ≤1 Position: {close_matches} out of 10\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the analysis\n",
        "sample_results = analyze_sample_rankings(X_test, y_test, df, ranker)\n",
        "\n",
        "# Calculate overall NDCG scores\n",
        "predictions_all = ranker.predict(X_test)\n",
        "for k in [5, 10, 20]:\n",
        "    ndcg = ndcg_score(\n",
        "        y_true=y_test.values.reshape(1, -1),\n",
        "        y_score=predictions_all.reshape(1, -1),\n",
        "        k=k\n",
        "    )\n",
        "    print(f\"\\nNDCG@{k}: {ndcg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBRKi7_4PPRf",
        "outputId": "822152fd-43c4-479d-ee48-2d34f31fae9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000954 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2099\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "Comparison of Rankings for 10 Random Papers:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 2 | Predicted Rank: 2\n",
            "Title: Crowd Sensing Based Semantic Annotation of Surveillance Videos\n",
            "True Score: 100.0 | Predicted Score: -5.63\n",
            "Features: IF:1.90, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted Rank: 4\n",
            "Title: STAT3 Regulates Steady-State Expression of Synaptopodin in Cultured Mouse Podocytes\n",
            "True Score: 100.0 | Predicted Score: -5.83\n",
            "Features: IF:3.20, Alt:6.04, Cit:5.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted Rank: 1\n",
            "Title: Effects of eicosapentaenoic acid and docosahexaenoic acid on prostate cancer cell migration and invasion induced by tumor-associated macrophages\n",
            "True Score: 100.0 | Predicted Score: 3.58\n",
            "Features: IF:2.90, Alt:13.85, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted Rank: 3\n",
            "Title: Safety and efficacy of dapoxetine in the treatment of premature ejaculation: a double-blind, placebo-controlled, fixed-dose, randomized study\n",
            "True Score: 100.0 | Predicted Score: -5.82\n",
            "Features: IF:6.60, Alt:11.00, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 5 | Predicted Rank: 6\n",
            "Title: NF-kB activation impairs somatic cell reprogramming in ageing\n",
            "True Score: 75.0 | Predicted Score: -5.92\n",
            "Features: IF:17.30, Alt:86.76, Cit:61.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted Rank: 5\n",
            "Title: Abstraction for data integration:Fusing mammalian molecular, cellular and phenotype big datasets for better knowledge extraction\n",
            "True Score: 48.0 | Predicted Score: -5.85\n",
            "Features: IF:2.60, Alt:2.00, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted Rank: 9\n",
            "Title: Socially dominant women strategically build coalitions of strong men in resource-rich environments\n",
            "True Score: 45.0 | Predicted Score: -6.31\n",
            "Features: IF:3.50, Alt:8.54, Cit:4.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted Rank: 7\n",
            "Title: Radiology Residents' Attitudes Toward Recurrent RADPAC Political Contributions: Current Data and Implications for the Future\n",
            "True Score: 40.0 | Predicted Score: -6.24\n",
            "Features: IF:4.00, Alt:4.85, Cit:9.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted Rank: 8\n",
            "Title: Complementary Therapies as a Strategy to Reduce Stress and Stimulate Immunity of Women With Breast Cancer\n",
            "True Score: 40.0 | Predicted Score: -6.31\n",
            "Features: IF:3.30, Alt:0.50, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted Rank: 10\n",
            "Title: Enhancement of RNA-directed DNA methylation of a transgene by simultaneously downregulating a ROS1 ortholog using a virus vector in Nicotiana benthamiana\n",
            "True Score: 30.0 | Predicted Score: -6.31\n",
            "Features: IF:2.80, Alt:8.29, Cit:4.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Rank Correlation for these 10 papers: 0.916\n",
            "Perfect Rank Matches: 1 out of 10\n",
            "Ranks Off by ≤1 Position: 6 out of 10\n",
            "\n",
            "NDCG@5: 1.0000\n",
            "\n",
            "NDCG@10: 1.0000\n",
            "\n",
            "NDCG@20: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_multiple_runs(X, y_groups, df, n_runs=10):\n",
        "    performance_metrics = {\n",
        "        'ndcg_5': [],\n",
        "        'ndcg_10': [],\n",
        "        'ndcg_20': [],\n",
        "        'spearman_corr': [],\n",
        "        'perfect_matches': [],\n",
        "        'close_matches': []\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "\n",
        "        # Split data with different random state for each run\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_groups, test_size=0.2, random_state=run, stratify=y_groups\n",
        "        )\n",
        "\n",
        "        # Create query groups\n",
        "        train_query_groups = create_query_groups(y_train)\n",
        "        test_query_groups = create_query_groups(y_test)\n",
        "\n",
        "        # Train model\n",
        "        ranker = LGBMRanker(\n",
        "            objective=\"lambdarank\",\n",
        "            metric=\"ndcg\",\n",
        "            boosting_type=\"gbdt\",\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.03,\n",
        "            max_depth=8,\n",
        "            num_leaves=31,\n",
        "            importance_type=\"gain\",\n",
        "            random_state=run,\n",
        "            label_gain=list(range(8))\n",
        "        )\n",
        "\n",
        "        ranker.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            group=train_query_groups,\n",
        "            eval_set=[(X_test, y_test)],\n",
        "            eval_group=[test_query_groups],\n",
        "            eval_metric='ndcg'\n",
        "        )\n",
        "\n",
        "        # Get predictions\n",
        "        predictions_all = ranker.predict(X_test)\n",
        "\n",
        "        # Calculate NDCG scores\n",
        "        for k in [5, 10, 20]:\n",
        "            ndcg = ndcg_score(\n",
        "                y_true=y_test.values.reshape(1, -1),\n",
        "                y_score=predictions_all.reshape(1, -1),\n",
        "                k=k\n",
        "            )\n",
        "            performance_metrics[f'ndcg_{k}'].append(ndcg)\n",
        "\n",
        "        # Sample 10 random papers for detailed metrics\n",
        "        random_indices = np.random.choice(len(X_test), 10, replace=False)\n",
        "        X_sample = X_test.iloc[random_indices]\n",
        "        sample_predictions = ranker.predict(X_sample)\n",
        "\n",
        "        # Calculate ranking metrics\n",
        "        results = pd.DataFrame({\n",
        "            'Predicted_Score': sample_predictions,\n",
        "            'Actual_Score': df['reason_score_100'].iloc[X_sample.index]\n",
        "        })\n",
        "\n",
        "        results['Predicted_Rank'] = results['Predicted_Score'].rank(ascending=False)\n",
        "        results['True_Rank'] = results['Actual_Score'].rank(ascending=False)\n",
        "\n",
        "        # Calculate correlation and matches\n",
        "        spearman = results['Predicted_Rank'].corr(results['True_Rank'], method='spearman')\n",
        "        perfect = (results['Predicted_Rank'] == results['True_Rank']).sum()\n",
        "        close = (abs(results['Predicted_Rank'] - results['True_Rank']) <= 1).sum()\n",
        "\n",
        "        performance_metrics['spearman_corr'].append(spearman)\n",
        "        performance_metrics['perfect_matches'].append(perfect)\n",
        "        performance_metrics['close_matches'].append(close)\n",
        "\n",
        "        # Print current run metrics\n",
        "        print(f\"NDCG@10: {performance_metrics['ndcg_10'][-1]:.4f}\")\n",
        "        print(f\"Spearman: {spearman:.3f}\")\n",
        "        print(f\"Perfect/Close Matches: {perfect}/{close} out of 10\")\n",
        "\n",
        "    # Calculate and print average metrics\n",
        "    print(\"\\nAverage Performance Over\", n_runs, \"Runs:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"NDCG@5: {np.mean(performance_metrics['ndcg_5']):.4f} ± {np.std(performance_metrics['ndcg_5']):.4f}\")\n",
        "    print(f\"NDCG@10: {np.mean(performance_metrics['ndcg_10']):.4f} ± {np.std(performance_metrics['ndcg_10']):.4f}\")\n",
        "    print(f\"NDCG@20: {np.mean(performance_metrics['ndcg_20']):.4f} ± {np.std(performance_metrics['ndcg_20']):.4f}\")\n",
        "    print(f\"Spearman Correlation: {np.mean(performance_metrics['spearman_corr']):.3f} ± {np.std(performance_metrics['spearman_corr']):.3f}\")\n",
        "    print(f\"Perfect Matches: {np.mean(performance_metrics['perfect_matches']):.1f} ± {np.std(performance_metrics['perfect_matches']):.1f} out of 10\")\n",
        "    print(f\"Close Matches: {np.mean(performance_metrics['close_matches']):.1f} ± {np.std(performance_metrics['close_matches']):.1f} out of 10\")\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "# Run multiple evaluations\n",
        "performance_metrics = evaluate_multiple_runs(X, y_groups, df, n_runs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHSzxnOzQBQy",
        "outputId": "27ee2bc5-cd30-41a7-809a-d996659332a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2096\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.634\n",
            "Perfect/Close Matches: 3/4 out of 10\n",
            "\n",
            "Run 2/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000925 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2087\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.629\n",
            "Perfect/Close Matches: 0/3 out of 10\n",
            "\n",
            "Run 3/10\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000327 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2088\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.559\n",
            "Perfect/Close Matches: 0/5 out of 10\n",
            "\n",
            "Run 4/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000912 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2086\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.671\n",
            "Perfect/Close Matches: 3/6 out of 10\n",
            "\n",
            "Run 5/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000888 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2078\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.740\n",
            "Perfect/Close Matches: 2/6 out of 10\n",
            "\n",
            "Run 6/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000958 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2081\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.485\n",
            "Perfect/Close Matches: 0/2 out of 10\n",
            "\n",
            "Run 7/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000953 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2078\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.703\n",
            "Perfect/Close Matches: 1/5 out of 10\n",
            "\n",
            "Run 8/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001458 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2085\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.640\n",
            "Perfect/Close Matches: 1/4 out of 10\n",
            "\n",
            "Run 9/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001628 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2087\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.201\n",
            "Perfect/Close Matches: 1/4 out of 10\n",
            "\n",
            "Run 10/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2083\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 9\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 1.0000\n",
            "Spearman: 0.844\n",
            "Perfect/Close Matches: 0/5 out of 10\n",
            "\n",
            "Average Performance Over 10 Runs:\n",
            "==================================================\n",
            "NDCG@5: 1.0000 ± 0.0000\n",
            "NDCG@10: 1.0000 ± 0.0000\n",
            "NDCG@20: 1.0000 ± 0.0000\n",
            "Spearman Correlation: 0.611 ± 0.165\n",
            "Perfect Matches: 1.1 ± 1.1 out of 10\n",
            "Close Matches: 4.4 ± 1.2 out of 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMRanker\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "# Convert scores to 1-100 scale\n",
        "df['reason_score_100'] = (df['reason_score'] * 10).clip(1, 100)\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert string numbers to float\n",
        "    def clean_numeric(x):\n",
        "        if pd.isna(x):\n",
        "            return 0\n",
        "        if isinstance(x, str):\n",
        "            if x.startswith('<'):\n",
        "                return float(x[1:])\n",
        "            try:\n",
        "                return float(x)\n",
        "            except:\n",
        "                return 0\n",
        "        return x\n",
        "\n",
        "    # Clean numeric columns\n",
        "    numeric_features = ['Impact_Factor', 'original_altmetric_score', 'original_cited_by_posts_count']\n",
        "    for col in numeric_features:\n",
        "        df[col] = df[col].apply(clean_numeric)\n",
        "\n",
        "    # Create percentile ranks\n",
        "    df['impact_percentile'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['altmetric_percentile'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['citation_percentile'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Create log-transformed features (add 1 to handle zeros)\n",
        "    df['log_impact'] = np.log1p(df['Impact_Factor'])\n",
        "    df['log_altmetric'] = np.log1p(df['original_altmetric_score'])\n",
        "    df['log_citations'] = np.log1p(df['original_cited_by_posts_count'])\n",
        "\n",
        "    # Interaction features\n",
        "    df['impact_citation_interaction'] = df['impact_percentile'] * df['citation_percentile']\n",
        "    df['impact_altmetric_interaction'] = df['impact_percentile'] * df['altmetric_percentile']\n",
        "\n",
        "    # Ratio features with smoothing\n",
        "    epsilon = 1e-5\n",
        "    df['citation_impact_ratio'] = df['original_cited_by_posts_count'] / (df['Impact_Factor'] + epsilon)\n",
        "    df['altmetric_impact_ratio'] = df['original_altmetric_score'] / (df['Impact_Factor'] + epsilon)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "# Enhanced feature set\n",
        "features = [\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count',\n",
        "    'log_impact',\n",
        "    'log_altmetric',\n",
        "    'log_citations',\n",
        "    'impact_percentile',\n",
        "    'altmetric_percentile',\n",
        "    'citation_percentile',\n",
        "    'impact_citation_interaction',\n",
        "    'impact_altmetric_interaction',\n",
        "    'citation_impact_ratio',\n",
        "    'altmetric_impact_ratio'\n",
        "]\n",
        "\n",
        "# Create severity groups with exponential binning\n",
        "def create_severity_groups(scores, n_groups=10):\n",
        "    exp_scores = np.exp(scores / 25)  # Exponential transformation\n",
        "    return pd.qcut(exp_scores, q=n_groups, labels=False, duplicates='drop')\n",
        "\n",
        "# Prepare data\n",
        "X = df_engineered[features].copy()\n",
        "y_groups = create_severity_groups(df['reason_score_100'])\n",
        "\n",
        "def evaluate_multiple_runs(X, y_groups, df, n_runs=10):\n",
        "    performance_metrics = {\n",
        "        'ndcg_5': [], 'ndcg_10': [], 'ndcg_20': [],\n",
        "        'spearman_corr': [], 'perfect_matches': [], 'close_matches': []\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "\n",
        "        # Stratified split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_groups,\n",
        "            test_size=0.2,\n",
        "            random_state=run*42,\n",
        "            stratify=y_groups\n",
        "        )\n",
        "\n",
        "        # Simpler query groups based on unique values\n",
        "        def create_query_groups(y):\n",
        "            unique_vals = np.unique(y)\n",
        "            return np.array([np.sum(y == val) for val in unique_vals])\n",
        "\n",
        "        train_query_groups = create_query_groups(y_train)\n",
        "        test_query_groups = create_query_groups(y_test)\n",
        "\n",
        "        # Initialize model with simplified parameters\n",
        "        ranker = LGBMRanker(\n",
        "            objective=\"lambdarank\",\n",
        "            metric=\"ndcg\",\n",
        "            boosting_type=\"gbdt\",\n",
        "            n_estimators=500,\n",
        "            learning_rate=0.01,\n",
        "            max_depth=6,\n",
        "            num_leaves=31,\n",
        "            random_state=run*42,\n",
        "            label_gain=list(range(10))\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        ranker.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            group=train_query_groups,\n",
        "            eval_set=[(X_test, y_test)],\n",
        "            eval_group=[test_query_groups],\n",
        "            eval_metric='ndcg'\n",
        "        )\n",
        "\n",
        "        # Calculate metrics\n",
        "        predictions_all = ranker.predict(X_test)\n",
        "\n",
        "        # NDCG scores\n",
        "        for k in [5, 10, 20]:\n",
        "            ndcg = ndcg_score(\n",
        "                y_true=y_test.values.reshape(1, -1),\n",
        "                y_score=predictions_all.reshape(1, -1),\n",
        "                k=k\n",
        "            )\n",
        "            performance_metrics[f'ndcg_{k}'].append(ndcg)\n",
        "\n",
        "        # Sample analysis\n",
        "        random_indices = np.random.choice(len(X_test), 10, replace=False)\n",
        "        X_sample = X_test.iloc[random_indices]\n",
        "        sample_predictions = ranker.predict(X_sample)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            'Predicted_Score': sample_predictions,\n",
        "            'Actual_Score': df['reason_score_100'].iloc[X_sample.index]\n",
        "        })\n",
        "\n",
        "        results['Predicted_Rank'] = results['Predicted_Score'].rank(ascending=False)\n",
        "        results['True_Rank'] = results['Actual_Score'].rank(ascending=False)\n",
        "\n",
        "        spearman = results['Predicted_Rank'].corr(results['True_Rank'], method='spearman')\n",
        "        perfect = (results['Predicted_Rank'] == results['True_Rank']).sum()\n",
        "        close = (abs(results['Predicted_Rank'] - results['True_Rank']) <= 1).sum()\n",
        "\n",
        "        performance_metrics['spearman_corr'].append(spearman)\n",
        "        performance_metrics['perfect_matches'].append(perfect)\n",
        "        performance_metrics['close_matches'].append(close)\n",
        "\n",
        "        print(f\"NDCG@10: {performance_metrics['ndcg_10'][-1]:.4f}\")\n",
        "        print(f\"Spearman: {spearman:.3f}\")\n",
        "        print(f\"Perfect/Close Matches: {perfect}/{close} out of 10\")\n",
        "\n",
        "    # Print average metrics\n",
        "    print(\"\\nAverage Performance Over\", n_runs, \"Runs:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for metric in performance_metrics:\n",
        "        values = performance_metrics[metric]\n",
        "        mean = np.mean(values)\n",
        "        std = np.std(values)\n",
        "\n",
        "        if 'ndcg' in metric:\n",
        "            print(f\"{metric.upper()}: {mean:.4f} ± {std:.4f}\")\n",
        "        elif metric == 'spearman_corr':\n",
        "            print(f\"Spearman Correlation: {mean:.3f} ± {std:.3f}\")\n",
        "        else:\n",
        "            print(f\"{metric.replace('_', ' ').title()}: {mean:.1f} ± {std:.1f} out of 10\")\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "# Run multiple evaluations\n",
        "performance_metrics = evaluate_multiple_runs(X, y_groups, df, n_runs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4DtVgqcRfob",
        "outputId": "5c6c578d-ec9c-4586-a012-7eb0555bdbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001345 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3018\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.7563\n",
            "Spearman: 0.067\n",
            "Perfect/Close Matches: 1/4 out of 10\n",
            "\n",
            "Run 2/10\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3003\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.7159\n",
            "Spearman: -0.071\n",
            "Perfect/Close Matches: 2/3 out of 10\n",
            "\n",
            "Run 3/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002393 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3006\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.9032\n",
            "Spearman: -0.349\n",
            "Perfect/Close Matches: 0/0 out of 10\n",
            "\n",
            "Run 4/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001388 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2997\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.8539\n",
            "Spearman: -0.032\n",
            "Perfect/Close Matches: 0/3 out of 10\n",
            "\n",
            "Run 5/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002338 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3006\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.8886\n",
            "Spearman: 0.176\n",
            "Perfect/Close Matches: 1/3 out of 10\n",
            "\n",
            "Run 6/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002235 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3012\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.7298\n",
            "Spearman: -0.498\n",
            "Perfect/Close Matches: 2/2 out of 10\n",
            "\n",
            "Run 7/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001416 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3024\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.7937\n",
            "Spearman: 0.500\n",
            "Perfect/Close Matches: 0/5 out of 10\n",
            "\n",
            "Run 8/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001392 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.6360\n",
            "Spearman: -0.246\n",
            "Perfect/Close Matches: 0/0 out of 10\n",
            "\n",
            "Run 9/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001460 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3012\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.9010\n",
            "Spearman: 0.293\n",
            "Perfect/Close Matches: 0/3 out of 10\n",
            "\n",
            "Run 10/10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002412 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3006\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "NDCG@10: 0.6918\n",
            "Spearman: 0.239\n",
            "Perfect/Close Matches: 3/6 out of 10\n",
            "\n",
            "Average Performance Over 10 Runs:\n",
            "==================================================\n",
            "NDCG_5: 0.8018 ± 0.1426\n",
            "NDCG_10: 0.7870 ± 0.0908\n",
            "NDCG_20: 0.7766 ± 0.0592\n",
            "Spearman Correlation: 0.008 ± 0.294\n",
            "Perfect Matches: 0.9 ± 1.0 out of 10\n",
            "Close Matches: 2.9 ± 1.8 out of 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMRanker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Clean numeric features\n",
        "    numeric_features = ['Impact_Factor', 'original_altmetric_score', 'original_cited_by_posts_count']\n",
        "    for col in numeric_features:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Create ratios and interactions\n",
        "    df['if_alt_ratio'] = df['Impact_Factor'] / (df['original_altmetric_score'] + 1)\n",
        "    df['if_cit_ratio'] = df['Impact_Factor'] / (df['original_cited_by_posts_count'] + 1)\n",
        "    df['alt_cit_ratio'] = df['original_altmetric_score'] / (df['original_cited_by_posts_count'] + 1)\n",
        "\n",
        "    # Create percentile ranks\n",
        "    df['if_rank'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['alt_rank'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['cit_rank'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Create composite scores\n",
        "    df['impact_score'] = (df['if_rank'] + df['alt_rank'] + df['cit_rank']) / 3\n",
        "    df['weighted_impact'] = (df['if_rank'] * 0.4 + df['alt_rank'] * 0.3 + df['cit_rank'] * 0.3)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Convert scores to 1-100 scale\n",
        "df['reason_score_100'] = (df['reason_score'] * 10).clip(1, 100)\n",
        "\n",
        "# Apply feature engineering\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "# Define features\n",
        "features = [\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count',\n",
        "    'if_alt_ratio',\n",
        "    'if_cit_ratio',\n",
        "    'alt_cit_ratio',\n",
        "    'if_rank',\n",
        "    'alt_rank',\n",
        "    'cit_rank',\n",
        "    'impact_score',\n",
        "    'weighted_impact'\n",
        "]\n",
        "\n",
        "# Create severity groups with proper group creation\n",
        "def create_query_groups(y):\n",
        "    # Count number of samples in each severity level\n",
        "    unique_vals = np.unique(y)\n",
        "    groups = []\n",
        "\n",
        "    for val in unique_vals:\n",
        "        count = np.sum(y == val)\n",
        "        if count > 0:\n",
        "            groups.append(count)\n",
        "\n",
        "    return np.array(groups)\n",
        "\n",
        "class EnsembleRanker:\n",
        "    def __init__(self, n_models=5):\n",
        "        self.n_models = n_models\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y, groups):\n",
        "        for i in range(self.n_models):\n",
        "            model = LGBMRanker(\n",
        "                objective=\"lambdarank\",\n",
        "                metric=\"ndcg\",\n",
        "                n_estimators=100,\n",
        "                learning_rate=0.05,\n",
        "                max_depth=5,\n",
        "                num_leaves=16,\n",
        "                min_data_in_leaf=20,\n",
        "                feature_fraction=0.8,\n",
        "                bagging_fraction=0.8,\n",
        "                bagging_freq=5,\n",
        "                random_state=i*42\n",
        "            )\n",
        "            # Removed verbose parameter\n",
        "            model.fit(\n",
        "                X, y,\n",
        "                group=groups\n",
        "            )\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for model in self.models:\n",
        "            pred = model.predict(X)\n",
        "            predictions.append(pred)\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "def run_evaluation(n_runs=10):\n",
        "    all_metrics = {\n",
        "        'ndcg_5': [], 'ndcg_10': [], 'ndcg_20': [],\n",
        "        'spearman': [], 'perfect': [], 'close': []\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=run*42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Create query groups after split\n",
        "        train_groups = create_query_groups(y_train)\n",
        "        test_groups = create_query_groups(y_test)\n",
        "\n",
        "        # Train ensemble and get predictions\n",
        "        ensemble = EnsembleRanker(n_models=5)\n",
        "        ensemble.fit(X_train, y_train, train_groups)\n",
        "        predictions = ensemble.predict(X_test)\n",
        "\n",
        "        # Evaluate on random sample\n",
        "        results = evaluate_rankings(X_test, y_test, df, predictions)\n",
        "\n",
        "        # Calculate metrics\n",
        "        for k in [5, 10, 20]:\n",
        "            ndcg = ndcg_score(\n",
        "                y_true=y_test.values.reshape(1, -1),\n",
        "                y_score=predictions.reshape(1, -1),\n",
        "                k=k\n",
        "            )\n",
        "            all_metrics[f'ndcg_{k}'].append(ndcg)\n",
        "\n",
        "        spearman = results['Predicted_Rank'].corr(results['True_Rank'], method='spearman')\n",
        "        perfect = (results['Predicted_Rank'] == results['True_Rank']).sum()\n",
        "        close = (abs(results['Predicted_Rank'] - results['True_Rank']) <= 1).sum()\n",
        "\n",
        "        all_metrics['spearman'].append(spearman)\n",
        "        all_metrics['perfect'].append(perfect)\n",
        "        all_metrics['close'].append(close)\n",
        "\n",
        "    print(\"\\nAverage Performance:\")\n",
        "    print(\"=\" * 50)\n",
        "    for metric, values in all_metrics.items():\n",
        "        mean = np.mean(values)\n",
        "        std = np.std(values)\n",
        "        if 'ndcg' in metric:\n",
        "            print(f\"{metric.upper()}: {mean:.4f} ± {std:.4f}\")\n",
        "        elif metric == 'spearman':\n",
        "            print(f\"Spearman Correlation: {mean:.3f} ± {std:.3f}\")\n",
        "        else:\n",
        "            print(f\"{metric.title()}: {mean:.1f} ± {std:.1f} out of 10\")\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "# Run the evaluation\n",
        "metrics = run_evaluation(n_runs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skocy6K3TfdN",
        "outputId": "2cb23605-39d3-42ee-949f-5280c7e4a931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002296 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002215 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002449 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002273 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001333 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 1 | Predicted: 9\n",
            "Title: Inhibition of HDAC4 attenuated JNK/c-Jun-dependent neuronal apoptosis and early brain injury following subarachnoid hemorrhage by transcriptionally suppressing MKK7\n",
            "Score: 100.0 vs -1.12\n",
            "Features: IF:4.20, Alt:1.25, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 1 | Predicted: 10\n",
            "Title: Hepatoprotective effects on alcoholic liver disease of fermented silkworms with Bacillus subtilis and Aspergillus kawachii\n",
            "Score: 100.0 vs -1.36\n",
            "Features: IF:3.50, Alt:1.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 8\n",
            "Title: Long non-coding RNA LINC00525 regulates the proliferation and epithelial to mesenchymal transition of human glioma cells by sponging miR-338-3p\n",
            "Score: 94.0 vs -1.03\n",
            "Features: IF:3.50, Alt:1.25, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 3\n",
            "Title: Activation of TGF-ß1 Promoter by Hepatitis C Virus-Induced AP-1 and Sp1: Role of TGF-ß1 in Hepatic Stellate Cell Activation and Invasion\n",
            "Score: 93.0 vs -0.35\n",
            "Features: IF:2.90, Alt:0.50, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 5 | Predicted: 6\n",
            "Title: Oxidative stress influences cholesterol efflux in THP-1 macrophages: Role of ATP-binding cassette A1 and nuclear factors\n",
            "Score: 86.0 vs -0.85\n",
            "Features: IF:10.20, Alt:5.08, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 1\n",
            "Title: Dissection of Soybean Populations According to Selection Signatures Based on Whole-Genome Sequences\n",
            "Score: 85.0 vs 0.67\n",
            "Features: IF:11.80, Alt:6.20, Cit:14.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted: 4\n",
            "Title: Effect of temperature on yield and properties of the sub-fractions derived from pyrolysis of Calophyllum inophyllum deoiled cake\n",
            "Score: 65.0 vs -0.60\n",
            "Features: IF:9.00, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 7\n",
            "Title: The TSG101 Tumor Susceptibility Gene Is Located in Chromosome 11 Band p15 and Is Mutated in Human Breast Cancer\n",
            "Score: 50.0 vs -0.95\n",
            "Features: IF:45.50, Alt:15.00, Cit:11.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 5\n",
            "Title: RADseq Data Suggest Occasional Hybridization between Microcebus murinus and M. ravelobensis in Northwestern Madagascar\n",
            "Score: 50.0 vs -0.62\n",
            "Features: IF:2.80, Alt:0.75, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted: 2\n",
            "Title: Serum-free-medium-type mesenchymal stem cell culture supernatant exerts a protective effect on A549 lung epithelial cells in acute lung injury induced by H2O2\n",
            "Score: 35.0 vs 0.53\n",
            "Features: IF:3.80, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: -0.634\n",
            "Perfect Matches: 0 out of 10\n",
            "Close Matches: 2 out of 10\n",
            "\n",
            "Run 2/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001351 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001393 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001349 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001428 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001431 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 1 | Predicted: 4\n",
            "Title: Long Non-Coding RNA LINC01783 Promotes the Progression of Cervical Cancer by Sponging miR-199b-5p to Mediate GBP1 Expression\n",
            "Score: 100.0 vs -1.05\n",
            "Features: IF:2.50, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 7\n",
            "Title: Differential Evolutionary Multiple-Objective Sequential Optimization of a Power Delivery Network\n",
            "Score: 95.0 vs -1.38\n",
            "Features: IF:2.00, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 6\n",
            "Title: Mimicking Biological Sensorimotor Functionalities by Combined Artificial Optical Synaptic and Actuating Components\n",
            "Score: 83.0 vs -1.35\n",
            "Features: IF:6.80, Alt:1.85, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 5\n",
            "Title: Neuromuscular Specializations of the Human Hypopharyngeal Muscles\n",
            "Score: 73.0 vs -1.27\n",
            "Features: IF:2.20, Alt:0.50, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 5 | Predicted: 1\n",
            "Title: A shorter 146Sm half-life measured and implications for 146Sm-142Nd chronology in the Solar System\n",
            "Score: 50.0 vs -0.13\n",
            "Features: IF:44.70, Alt:32.41, Cit:18.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 3\n",
            "Title: A Systematic Review and Meta-analysis of Feasibility, Safety and Efficacy of Associating Liver Partition and Portal Vein Ligation for Staged Hepatectomy (ALPPS) Versus Two-stage Hepatectomy (TSH)\n",
            "Score: 45.0 vs -1.04\n",
            "Features: IF:5.70, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 2\n",
            "Title: Experience with tumescent technique in lipoplasty\n",
            "Score: 45.0 vs -1.03\n",
            "Features: IF:2.00, Alt:3.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 10\n",
            "Title: The Arabidopsis NOT4A E3 ligase promotes PGR3 expression and regulates chloroplast translation\n",
            "Score: 35.0 vs -1.95\n",
            "Features: IF:14.70, Alt:42.58, Cit:60.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 9\n",
            "Title: Neglecting nonlocality leads to unreliable numerical methods for fractional differential equations\n",
            "Score: 25.0 vs -1.60\n",
            "Features: IF:3.40, Alt:8.83, Cit:5.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted: 8\n",
            "Title: Modeling and Simulation of China Railway High-Speed Electric Multiple Units Based on Multi-Agent\n",
            "Score: 15.0 vs -1.40\n",
            "Features: IF:10.10, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: 0.389\n",
            "Perfect Matches: 1 out of 10\n",
            "Close Matches: 2 out of 10\n",
            "\n",
            "Run 3/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002301 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3003\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001341 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3003\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001362 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3003\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3003\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001381 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3003\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 2 | Predicted: 7\n",
            "Title: A Modified Version of Galectin-9 Suppresses Cell Growth and Induces Apoptosis of Human T-cell Leukemia Virus Type I-infected T-cell Lines\n",
            "Score: 100.0 vs -1.61\n",
            "Features: IF:5.70, Alt:6.25, Cit:13.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 8\n",
            "Title: Probiotics: a comprehensive approach toward health foods\n",
            "Score: 100.0 vs -1.66\n",
            "Features: IF:7.30, Alt:1.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 10\n",
            "Title: A specific amyloid-β protein assembly in the brain impairs memory\n",
            "Score: 100.0 vs -2.24\n",
            "Features: IF:50.50, Alt:1784.85, Cit:788.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 6\n",
            "Title: Star Trek, medicine, ethics, nanotechnology and nursing\n",
            "Score: 83.0 vs -1.53\n",
            "Features: IF:2.20, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 5 | Predicted: 1\n",
            "Title: Glioma Grading: Sensitivity, Specificity, Positive and Negative Predictive Values of Diffusion and Perfusion Imaging\n",
            "Score: 73.0 vs -0.22\n",
            "Features: IF:3.20, Alt:12.00, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 2\n",
            "Title: Transcription factor Krüppel-like factor 2 plays a vital role in endothelial colony forming cells differentiation\n",
            "Score: 68.0 vs -0.51\n",
            "Features: IF:10.20, Alt:11.00, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted: 4\n",
            "Title: Pumpkin Oil-Based Nanostructured Lipid Carrier System for Antiulcer Effect in NSAID-Induced Gastric Ulcer Model in Rats\n",
            "Score: 65.0 vs -1.44\n",
            "Features: IF:6.60, Alt:5.08, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 9\n",
            "Title: 24-Hour Rhythms of DNA Methylation and Their Relation with Rhythms of RNA Expression in the Human Dorsolateral Prefrontal Cortex\n",
            "Score: 43.0 vs -1.98\n",
            "Features: IF:4.00, Alt:29.58, Cit:22.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 4\n",
            "Title: Antiviral Activity of Chitosan Nanoparticles Encapsulating Curcumin Against Hepatitis C Virus Genotype 4a in Human Hepatoma Cell Lines\n",
            "Score: 27.0 vs -1.44\n",
            "Features: IF:6.60, Alt:5.08, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted: 3\n",
            "Title: The Development of Willingness to Sacrifice and Unmitigated Communion in Intimate Partnerships\n",
            "Score: 25.0 vs -1.36\n",
            "Features: IF:2.70, Alt:4.00, Cit:9.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: -0.462\n",
            "Perfect Matches: 0 out of 10\n",
            "Close Matches: 1 out of 10\n",
            "\n",
            "Run 4/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001488 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001361 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001355 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001320 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002341 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 3 | Predicted: 1\n",
            "Title: Expression Levels and Clinical Significance of Serum miR-497, CEA, CA24-2, and HBsAg in Patients with Colorectal Cancer\n",
            "Score: 100.0 vs 0.57\n",
            "Features: IF:2.60, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 4\n",
            "Title: Readiness for Hospital Discharge and Its Correlation with the Quality of Discharge Teaching among the Parents of Premature Infants in NICU\n",
            "Score: 100.0 vs -1.21\n",
            "Features: IF:1.80, Alt:0.25, Cit:4.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 2\n",
            "Title: Oncolysis of pancreatic tumour cells by a γ34.5-deleted HSV-1 does not rely upon Ras-activation, but on the PI 3-kinase pathway\n",
            "Score: 100.0 vs -0.77\n",
            "Features: IF:4.60, Alt:5.08, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 3\n",
            "Title: Sustained Hypoxia Reduces GABAergic Modulation on NTS Neurons Sending Projections to Ventral Medulla of Rats\n",
            "Score: 100.0 vs -1.08\n",
            "Features: IF:2.90, Alt:1.25, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 7\n",
            "Title: Aortic Homograft Endocarditis Caused by Campylobacter Jejuni\n",
            "Score: 100.0 vs -1.57\n",
            "Features: IF:6.10, Alt:5.04, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 6\n",
            "Title: Cellulose microcrystalline: A promising ecofriendly approach to control Culex quinquefasciatus larvae\n",
            "Score: 100.0 vs -1.39\n",
            "Features: IF:8.20, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted: 9\n",
            "Title: Impact of Frequency-Dependent Terminations and Defect Impedances on the Accuracy of the Unmatched-Media Time Reversal Method\n",
            "Score: 95.0 vs -1.79\n",
            "Features: IF:2.00, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 8\n",
            "Title: Genome sequence and description of Anaerosalibacter massiliensis sp. nov.\n",
            "Score: 63.0 vs -1.65\n",
            "Features: IF:2.90, Alt:3.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 10\n",
            "Title: The evolutionary history of colour polymorphism in Ischnura damselflies\n",
            "Score: 50.0 vs -1.97\n",
            "Features: IF:2.10, Alt:8.51, Cit:6.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted: 5\n",
            "Title: Fructooligosaccharides: From Breast Milk Components to Potential Supplements. A Systematic Review\n",
            "Score: 15.0 vs -1.34\n",
            "Features: IF:8.00, Alt:15.08, Cit:11.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: 0.615\n",
            "Perfect Matches: 1 out of 10\n",
            "Close Matches: 4 out of 10\n",
            "\n",
            "Run 5/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002340 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3012\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002275 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3012\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002341 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3012\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001421 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3012\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3012\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 3 | Predicted: 2\n",
            "Title: Characterization of enhanced antibacterial effects of novel silver nanoparticles\n",
            "Score: 100.0 vs -0.94\n",
            "Features: IF:2.90, Alt:6.00, Cit:5.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 9\n",
            "Title: The effect of intellectual disability on the presence of comorbid symptoms in children and adolescents with autism spectrum disorder\n",
            "Score: 100.0 vs -1.37\n",
            "Features: IF:2.20, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 5\n",
            "Title: On the same origin of quantum physics and general relativity from Riemannian geometry and Planck scale formalism\n",
            "Score: 100.0 vs -1.19\n",
            "Features: IF:4.20, Alt:220.25, Cit:197.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 1\n",
            "Title: Identification of an immunologic signature of lung adenocarcinomas based on genome-wide immune expression profiles\n",
            "Score: 100.0 vs -0.74\n",
            "Features: IF:3.90, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 6\n",
            "Title: Field-induced superconductivity in a spin-ladder cuprate\n",
            "Score: 100.0 vs -1.21\n",
            "Features: IF:44.70, Alt:4.00, Cit:4.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 4\n",
            "Title: NF-κB signaling pathway inhibition suppresses hippocampal neuronal apoptosis and cognitive impairment via RCAN1 in neonatal rats with hypoxic-ischemic brain damage\n",
            "Score: 94.0 vs -1.18\n",
            "Features: IF:3.40, Alt:4.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 7\n",
            "Title: A 2D fluid motion model of the estuarine water circulation: Physical analysis of the salinity stratification in the Sebou estuary\n",
            "Score: 94.0 vs -1.22\n",
            "Features: IF:2.80, Alt:9.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 3\n",
            "Title: How negative experiences shape long-term food preferences. Fifty years from the World War II combat front\n",
            "Score: 75.0 vs -1.01\n",
            "Features: IF:4.60, Alt:18.12, Cit:4.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 10\n",
            "Title: A test of the variability vs. specificity hypotheses in the retention of a motor skill\n",
            "Score: 56.0 vs -1.64\n",
            "Features: IF:1.60, Alt:2.85, Cit:5.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted: 8\n",
            "Title: COVID-19: Mathematical estimation of delay to deaths in relation to upsurges\n",
            "Score: 51.0 vs -1.32\n",
            "Features: IF:2.20, Alt:24.29, Cit:14.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: 0.415\n",
            "Perfect Matches: 0 out of 10\n",
            "Close Matches: 3 out of 10\n",
            "\n",
            "Run 6/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001368 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001396 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001434 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001263 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001346 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 2 | Predicted: 4\n",
            "Title: Hypothalamic Inhibition of Acetyl-CoA Carboxylase Stimulates Hepatic Counter-Regulatory Response Independent of AMPK Activation in Rats\n",
            "Score: 100.0 vs -0.66\n",
            "Features: IF:2.90, Alt:1.00, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 7\n",
            "Title: An aberrant phosphorylation of amyloid precursor protein tyrosine regulates its trafficking and the binding to the clathrin endocytic complex in neural stem cells of Alzheimer's disease patients\n",
            "Score: 100.0 vs -1.54\n",
            "Features: IF:3.50, Alt:13.00, Cit:8.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 2\n",
            "Title: Effect of platelet-rich plasma on the healing of cutaneous defects exposed to acute to chronic wounds: a clinico-histopathologic study in rabbits\n",
            "Score: 100.0 vs -0.51\n",
            "Features: IF:2.40, Alt:1.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 1\n",
            "Title: Meta-analysis of the efficacy of pancreatoduodenectomy with extended lymphadenectomy in the treatment of pancreatic cancer\n",
            "Score: 100.0 vs 0.95\n",
            "Features: IF:2.50, Alt:7.50, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 5 | Predicted: 10\n",
            "Title: Epigenetic modification of the oxytocin gene is associated with gray matter volume and trait empathy in mothers\n",
            "Score: 95.0 vs -1.96\n",
            "Features: IF:3.40, Alt:139.83, Cit:81.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 8\n",
            "Title: Detection of Mycobacterium lepromatosis in patients with leprosy in India\n",
            "Score: 70.0 vs -1.55\n",
            "Features: IF:2.90, Alt:2.70, Cit:6.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted: 6\n",
            "Title: National construction and LGBTI rights: Exploring Catalan homonationalism\n",
            "Score: 65.0 vs -1.35\n",
            "Features: IF:2.10, Alt:2.35, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 5\n",
            "Title: Early Cretaceous origin of the Woyla Arc (Sumatra, Indonesia) on the Australian plate\n",
            "Score: 50.0 vs -0.99\n",
            "Features: IF:4.80, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 9\n",
            "Title: Preoperative statin therapy for patients undergoing cardiac surgery (Pub 2)\n",
            "Score: 48.0 vs -1.71\n",
            "Features: IF:8.80, Alt:11.00, Cit:15.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted: 3\n",
            "Title: The Scenario of Norovirus Contamination in Food and Food Handlers\n",
            "Score: 15.0 vs -0.59\n",
            "Features: IF:2.50, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: 0.294\n",
            "Perfect Matches: 1 out of 10\n",
            "Close Matches: 3 out of 10\n",
            "\n",
            "Run 7/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001345 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001409 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001386 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001361 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001386 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 3 | Predicted: 1\n",
            "Title: Crocus sativus L. (Saffron) Stigma Aqueous Extract Induces Apoptosis in Alveolar Human Lung Cancer Cells through Caspase-Dependent Pathways Activation\n",
            "Score: 100.0 vs 0.90\n",
            "Features: IF:2.60, Alt:10.45, Cit:22.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 2\n",
            "Title: Fetal safety of medications used in treating infertility\n",
            "Score: 100.0 vs -0.09\n",
            "Features: IF:3.60, Alt:5.08, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 3\n",
            "Title: Amelioration by mecobalamin of subclinical carpal tunnel syndrome involving unaffected limbs in stroke patients\n",
            "Score: 100.0 vs -0.38\n",
            "Features: IF:3.60, Alt:7.54, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 4\n",
            "Title: Air-stable superparamagnetic metal nanoparticles entrapped in graphene oxide matrix\n",
            "Score: 100.0 vs -0.79\n",
            "Features: IF:14.70, Alt:8.68, Cit:6.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 3 | Predicted: 7\n",
            "Title: Dexmedetomidine improves early postoperative neurocognitive disorder in elderly male patients undergoing thoracoscopic lobectomy\n",
            "Score: 100.0 vs -1.33\n",
            "Features: IF:2.40, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 8\n",
            "Title: Exposure to Magnetic Field Non-Ionizing Radiation and the Risk of Miscarriage: A Prospective Cohort Study\n",
            "Score: 75.0 vs -1.33\n",
            "Features: IF:3.80, Alt:870.35, Cit:384.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted: 6\n",
            "Title: Magneto transport measurements to unravel the epitaxial strain control of magnetic anisotropy in Ga1-xMnxAs/InGaAs layers\n",
            "Score: 65.0 vs -1.21\n",
            "Features: IF:2.50, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted: 5\n",
            "Title: Characteristics and electrical conductivity of graphene and graphene oxide for adsorption of cationic dyes from liquids: Kinetic and thermodynamic study\n",
            "Score: 65.0 vs -1.05\n",
            "Features: IF:5.90, Alt:1.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 9\n",
            "Title: Fiscal Uncertainty and Currency Crises\n",
            "Score: 48.0 vs -1.33\n",
            "Features: IF:2.00, Alt:3.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted: 10\n",
            "Title: Early Paleoindian Personal Adornment: An Example from the Brian D Jones Site in Avon, Connecticut\n",
            "Score: 40.0 vs -1.35\n",
            "Features: IF:1.60, Alt:5.08, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: 0.778\n",
            "Perfect Matches: 3 out of 10\n",
            "Close Matches: 5 out of 10\n",
            "\n",
            "Run 8/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001489 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001397 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002304 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002313 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002473 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3009\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 4 | Predicted: 10\n",
            "Title: The Nitrile-degrading Enzymes: Current Status and Future Prospects\n",
            "Score: 100.0 vs -1.65\n",
            "Features: IF:3.90, Alt:27.00, Cit:9.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 1\n",
            "Title: Indium-Mediated Catalytic Enantioselective Allylation of N-Benzoylhydrazones Using a Protonated Chiral Amine\n",
            "Score: 100.0 vs 0.59\n",
            "Features: IF:14.40, Alt:6.00, Cit:4.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 3\n",
            "Title: Small Interfering RNA Effectively Inhibits Protein Expression and Negative Strand RNA Synthesis From a Full-Length Hepatitis C Virus Clone\n",
            "Score: 100.0 vs -0.01\n",
            "Features: IF:6.80, Alt:8.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 2\n",
            "Title: Effect of drain placement in short-level spinal surgery on postoperative wound infection: A meta-analysis\n",
            "Score: 100.0 vs 0.36\n",
            "Features: IF:2.60, Alt:8.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 8\n",
            "Title: The Inhibitory Effect of Doxycycline on Cisplatin-Sensitive and -Resistant Epithelial Ovarian Cancer\n",
            "Score: 100.0 vs -1.20\n",
            "Features: IF:2.90, Alt:3.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 6\n",
            "Title: Calponin and caldesmon cellular domains in reacting microvessels following traumatic brain injury\n",
            "Score: 100.0 vs -0.38\n",
            "Features: IF:2.90, Alt:6.04, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 5\n",
            "Title: Clinical Value of Growth Differentiation Factor 15 Detection in the Diagnosis of Early Liver Cancer Based on Data Mining\n",
            "Score: 100.0 vs -0.35\n",
            "Features: IF:2.60, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 4\n",
            "Title: Robust optimization of renewable-based multi-energy micro-grid integrated with flexible energy conversion and storage devices\n",
            "Score: 40.0 vs -0.17\n",
            "Features: IF:10.50, Alt:5.08, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 9\n",
            "Title: Pneumonia in Vietnamese Children Aged 1 to 15 years Due to Atypical Pneumonia Causative Bacteria: Hospital-Based Microbiological and Epidemiological Characteristics\n",
            "Score: 15.0 vs -1.48\n",
            "Features: IF:1.30, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 7\n",
            "Title: Modeling of interval thermal processes in electronic systems at the interval uncertainty of the determinative parameters\n",
            "Score: 15.0 vs -0.79\n",
            "Features: IF:4.40, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: 0.322\n",
            "Perfect Matches: 0 out of 10\n",
            "Close Matches: 3 out of 10\n",
            "\n",
            "Run 9/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002482 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002341 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001285 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001363 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3000\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 2 | Predicted: 1\n",
            "Title: Meta-analysis on the efficacy of tourniquet on ankle trauma surgery.\n",
            "Score: 100.0 vs 1.34\n",
            "Features: IF:2.80, Alt:7.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 8\n",
            "Title: The Impact of Survivin on Prognosis and Clinicopathology of Glioma Patients: A Systematic Meta-Analysis\n",
            "Score: 100.0 vs -1.63\n",
            "Features: IF:4.60, Alt:5.04, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 3\n",
            "Title: Novel Target Genes Responsive to the Anti-growth Activity of Triptolide in Endometrial and Ovarian Cancer Cells\n",
            "Score: 100.0 vs -0.61\n",
            "Features: IF:9.10, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 4\n",
            "Title: Safety and immunogenicity of a recombinant receptor-binding domain-based protein subunit vaccine (Noora vaccine™) against COVID-19 in adults: A randomized, double-blind, placebo-controlled, Phase 1 trial\n",
            "Score: 100.0 vs -1.13\n",
            "Features: IF:6.80, Alt:17.00, Cit:8.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 5 | Predicted: 2\n",
            "Title: LncRNA LBX2-AS1 promotes colorectal cancer progression and 5-fluorouracil resistance\n",
            "Score: 88.0 vs -0.47\n",
            "Features: IF:5.30, Alt:8.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 5\n",
            "Title: Severe dopaminergic neurotoxicity in primates after a common recreational dose regimen of MDMA (ecstasy)\n",
            "Score: 85.0 vs -1.18\n",
            "Features: IF:44.70, Alt:94.43, Cit:53.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 7 | Predicted: 7\n",
            "Title: Effects of dexmedetomidine on postoperative delirium and expression of IL-1β, IL-6, and TNF-α in elderly patients after hip fracture operation\n",
            "Score: 64.0 vs -1.58\n",
            "Features: IF:4.40, Alt:0.50, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 9\n",
            "Title: ‘Khelaifiella massiliensis’, ‘Niameybacter massiliensis’, ‘Brachybacterium massiliense’, ‘Enterobacter timonensis’, ‘Massilibacillus massiliensis’, new bacterial species and genera isolated from the gut microbiota of healthy infants\n",
            "Score: 63.0 vs -1.73\n",
            "Features: IF:2.90, Alt:3.75, Cit:5.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 6\n",
            "Title: Epigenetic Silencing of E- and N-cadherins in the Stroma of Mouse Thymic Lymphomas\n",
            "Score: 25.0 vs -1.46\n",
            "Features: IF:3.30, Alt:1.00, Cit:5.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted: 10\n",
            "Title: First human case of Rickettsia sibirica mongolotimonae infection in Northern Greece in a teenager from Thrace\n",
            "Score: 15.0 vs -2.41\n",
            "Features: IF:1.30, Alt:3.95, Cit:7.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: 0.657\n",
            "Perfect Matches: 2 out of 10\n",
            "Close Matches: 5 out of 10\n",
            "\n",
            "Run 10/10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3027\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3027\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001522 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3027\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002527 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3027\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001426 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3027\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 13\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Ranking Comparison:\n",
            "====================================================================================================\n",
            "\n",
            "True Rank: 2 | Predicted: 6\n",
            "Title: PEG-nanotube liquid crystals as templates for construction of surfactant-free gold nanorods\n",
            "Score: 100.0 vs -1.05\n",
            "Features: IF:4.30, Alt:0.50, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 7\n",
            "Title: Crowd Sensing Based Burst Computing of Events Using Social Media\n",
            "Score: 100.0 vs -1.15\n",
            "Features: IF:1.90, Alt:0.25, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 2 | Predicted: 4\n",
            "Title: Ligand-Selective Potentiation of Rat Mineralocorticoid Receptor Activation Function 1 by a CBP-Containing Histone Acetyltransferase Complex\n",
            "Score: 100.0 vs -0.87\n",
            "Features: IF:3.20, Alt:9.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 4 | Predicted: 8\n",
            "Title: Differential Modification of p27(Kip1) Controls Its Cyclin D-cdk4 Inhibitory Activity\n",
            "Score: 92.0 vs -1.26\n",
            "Features: IF:3.20, Alt:12.04, Cit:12.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 5 | Predicted: 9\n",
            "Title: Modification of the acetylcholine-induced current of the snail Helix pomatia L. by fast temperature changes\n",
            "Score: 84.0 vs -1.51\n",
            "Features: IF:0.70, Alt:6.04, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 3\n",
            "Title: Y4lO of Rhizobium sp. Strain NGR234 Is a Symbiotic Determinant Required for Symbiosome Differentiation\n",
            "Score: 40.0 vs -0.85\n",
            "Features: IF:2.70, Alt:1.25, Cit:3.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 6 | Predicted: 1\n",
            "Title: LINC01638 lncRNA promotes the proliferation, migration and invasion of prostate carcinoma cells by interacting with Notch1\n",
            "Score: 40.0 vs -0.68\n",
            "Features: IF:2.20, Alt:8.00, Cit:1.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 8 | Predicted: 5\n",
            "Title: Organic Pools in the Subsea Permafrost Domain Since the Last Glacial Maximum\n",
            "Score: 30.0 vs -0.96\n",
            "Features: IF:4.60, Alt:13.00, Cit:6.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 9 | Predicted: 10\n",
            "Title: Nonfatal firearm injuries: Utilization and expenditures for children pre- and postinjury\n",
            "Score: 25.0 vs -1.51\n",
            "Features: IF:3.40, Alt:3.25, Cit:7.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "True Rank: 10 | Predicted: 2\n",
            "Title: Allogeneic Hematopoietic Stem Cell Transplantation for Acquired Aplastic Anemia Using Cyclophosphamide and Antithymocyte Globulin: A Single Center Experience\n",
            "Score: 15.0 vs -0.74\n",
            "Features: IF:4.50, Alt:9.50, Cit:2.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Spearman Correlation: -0.191\n",
            "Perfect Matches: 0 out of 10\n",
            "Close Matches: 1 out of 10\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.8814 ± 0.1322\n",
            "NDCG_10: 0.8292 ± 0.0973\n",
            "NDCG_20: 0.7939 ± 0.0880\n",
            "Spearman Correlation: 0.218 ± 0.459\n",
            "Perfect: 0.8 ± 1.0 out of 10\n",
            "Close: 2.9 ± 1.4 out of 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRanker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Clean numeric features\n",
        "    numeric_features = ['Impact_Factor', 'original_altmetric_score', 'original_cited_by_posts_count']\n",
        "    for col in numeric_features:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Create ratios and interactions with epsilon to avoid division by zero\n",
        "    df['if_alt_ratio'] = df['Impact_Factor'] / (df['original_altmetric_score'] + 1e-5)\n",
        "    df['if_cit_ratio'] = df['Impact_Factor'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "    df['alt_cit_ratio'] = df['original_altmetric_score'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Log transformations with epsilon\n",
        "    df['log_impact_factor'] = np.log1p(df['Impact_Factor'] + 1e-5)\n",
        "    df['log_altmetric'] = np.log1p(df['original_altmetric_score'] + 1e-5)\n",
        "    df['log_citations'] = np.log1p(df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Create percentile ranks\n",
        "    df['if_rank'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['alt_rank'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['cit_rank'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Create composite scores\n",
        "    df['impact_score'] = (df['if_rank'] + df['alt_rank'] + df['cit_rank']) / 3\n",
        "    df['weighted_impact'] = (df['if_rank'] * 0.4 + df['alt_rank'] * 0.3 + df['cit_rank'] * 0.3)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Convert scores to 1-100 scale\n",
        "df['reason_score_100'] = (df['reason_score'] * 10).clip(1, 100)\n",
        "\n",
        "# Apply feature engineering\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "# Define features\n",
        "features = [\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count',\n",
        "    'if_alt_ratio',\n",
        "    'if_cit_ratio',\n",
        "    'alt_cit_ratio',\n",
        "    'log_impact_factor',\n",
        "    'log_altmetric',\n",
        "    'log_citations',\n",
        "    'if_rank',\n",
        "    'alt_rank',\n",
        "    'cit_rank',\n",
        "    'impact_score',\n",
        "    'weighted_impact'\n",
        "]\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_engineered[features]\n",
        "y = df_engineered['reason_score_100']\n",
        "\n",
        "class XGBEnsembleRanker:\n",
        "    def __init__(self, n_models=5):\n",
        "        self.n_models = n_models\n",
        "        self.models = []\n",
        "        self.scalers = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i in range(self.n_models):\n",
        "            # Unique random state\n",
        "            np.random.seed(i*42)\n",
        "\n",
        "            # Scale features\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "            self.scalers.append(scaler)\n",
        "\n",
        "            # Create groups for ranking\n",
        "            unique_scores = np.unique(y)\n",
        "            group_sizes = [np.sum(y == score) for score in unique_scores]\n",
        "\n",
        "            model = XGBRanker(\n",
        "                objective='rank:pairwise',\n",
        "                learning_rate=0.05,\n",
        "                n_estimators=100,\n",
        "                max_depth=5,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=i*42\n",
        "            )\n",
        "\n",
        "            # Fit with groups\n",
        "            model.fit(\n",
        "                X_scaled,\n",
        "                y,\n",
        "                group=group_sizes,\n",
        "                verbose=False\n",
        "            )\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for scaler, model in zip(self.scalers, self.models):\n",
        "            X_scaled = scaler.transform(X)\n",
        "            pred = model.predict(X_scaled)\n",
        "            predictions.append(pred)\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "def run_evaluation(n_runs=10):\n",
        "    all_metrics = {\n",
        "        'ndcg_5': [], 'ndcg_10': [], 'ndcg_20': [],\n",
        "        'spearman': [], 'perfect': [], 'close': []\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=run*42\n",
        "        )\n",
        "\n",
        "        # Train ensemble and get predictions\n",
        "        ensemble = XGBEnsembleRanker(n_models=5)\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        predictions = ensemble.predict(X_test)\n",
        "\n",
        "        # Ensure predictions match test set length\n",
        "        predictions = predictions[:len(y_test)]\n",
        "\n",
        "        # Calculate NDCG\n",
        "        for k in [5, 10, 20]:\n",
        "            ndcg = ndcg_score(\n",
        "                y_true=y_test.values.reshape(1, -1),\n",
        "                y_score=predictions.reshape(1, -1),\n",
        "                k=k\n",
        "            )\n",
        "            all_metrics[f'ndcg_{k}'].append(ndcg)\n",
        "\n",
        "        # Detailed Ranking Analysis\n",
        "        results = pd.DataFrame({\n",
        "            'True_Score': y_test.reset_index(drop=True),\n",
        "            'Predicted_Score': pd.Series(predictions),\n",
        "            'True_Rank': y_test.reset_index(drop=True).rank(method='dense', ascending=False),\n",
        "            'Predicted_Rank': pd.Series(predictions).rank(method='dense', ascending=False)\n",
        "        })\n",
        "\n",
        "        # Compute Spearman correlation\n",
        "        spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n",
        "\n",
        "        perfect = (results['True_Rank'] == results['Predicted_Rank']).sum()\n",
        "        close = (abs(results['True_Rank'] - results['Predicted_Rank']) <= 1).sum()\n",
        "\n",
        "        all_metrics['spearman'].append(spearman_corr)\n",
        "        all_metrics['perfect'].append(perfect)\n",
        "        all_metrics['close'].append(close)\n",
        "\n",
        "    print(\"\\nAverage Performance:\")\n",
        "    print(\"=\" * 50)\n",
        "    for metric, values in all_metrics.items():\n",
        "        mean = np.mean(values)\n",
        "        std = np.std(values)\n",
        "        if 'ndcg' in metric:\n",
        "            print(f\"{metric.upper()}: {mean:.4f} ± {std:.4f}\")\n",
        "        elif metric == 'spearman':\n",
        "            print(f\"Spearman Correlation: {mean:.3f} ± {std:.3f}\")\n",
        "        else:\n",
        "            print(f\"{metric.title()}: {mean:.1f} ± {std:.1f} out of 10\")\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "# Run the evaluation\n",
        "metrics = run_evaluation(n_runs=10)\n",
        "\n",
        "# Optional: Print distribution of original scores\n",
        "print(\"\\nScore Distribution:\")\n",
        "print(y.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5ad8RVVbMZz",
        "outputId": "17e95c51-2134-4d9d-f825-5458d9b15636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1/10\n",
            "\n",
            "Run 2/10\n",
            "\n",
            "Run 3/10\n",
            "\n",
            "Run 4/10\n",
            "\n",
            "Run 5/10\n",
            "\n",
            "Run 6/10\n",
            "\n",
            "Run 7/10\n",
            "\n",
            "Run 8/10\n",
            "\n",
            "Run 9/10\n",
            "\n",
            "Run 10/10\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.8831 ± 0.0940\n",
            "NDCG_10: 0.8942 ± 0.0755\n",
            "NDCG_20: 0.8901 ± 0.0620\n",
            "Spearman Correlation: 0.217 ± 0.016\n",
            "Perfect: 2.5 ± 1.7 out of 10\n",
            "Close: 4.7 ± 2.7 out of 10\n",
            "\n",
            "Score Distribution:\n",
            "reason_score_100\n",
            "100.0    5097\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRanker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Clean numeric features\n",
        "    numeric_features = ['Impact_Factor', 'original_altmetric_score', 'original_cited_by_posts_count']\n",
        "    for col in numeric_features:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Create ratios and interactions with epsilon to avoid division by zero\n",
        "    df['if_alt_ratio'] = df['Impact_Factor'] / (df['original_altmetric_score'] + 1e-5)\n",
        "    df['if_cit_ratio'] = df['Impact_Factor'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "    df['alt_cit_ratio'] = df['original_altmetric_score'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Log transformations with epsilon\n",
        "    df['log_impact_factor'] = np.log1p(df['Impact_Factor'] + 1e-5)\n",
        "    df['log_altmetric'] = np.log1p(df['original_altmetric_score'] + 1e-5)\n",
        "    df['log_citations'] = np.log1p(df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Create percentile ranks\n",
        "    df['if_rank'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['alt_rank'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['cit_rank'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Create composite scores\n",
        "    df['impact_score'] = (df['if_rank'] + df['alt_rank'] + df['cit_rank']) / 3\n",
        "    df['weighted_impact'] = (df['if_rank'] * 0.4 + df['alt_rank'] * 0.3 + df['cit_rank'] * 0.3)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Convert scores to 1-100 scale\n",
        "df['reason_score_100'] = (df['reason_score'] * 10).clip(1, 100)\n",
        "\n",
        "# Apply feature engineering\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "# Define features\n",
        "features = [\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count',\n",
        "    'if_alt_ratio',\n",
        "    'if_cit_ratio',\n",
        "    'alt_cit_ratio',\n",
        "    'log_impact_factor',\n",
        "    'log_altmetric',\n",
        "    'log_citations',\n",
        "    'if_rank',\n",
        "    'alt_rank',\n",
        "    'cit_rank',\n",
        "    'impact_score',\n",
        "    'weighted_impact'\n",
        "]\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_engineered[features]\n",
        "y = df_engineered['reason_score_100']\n",
        "\n",
        "def handle_imbalanced_data(X, y, strategy='hybrid'):\n",
        "    \"\"\"\n",
        "    Handle imbalanced dataset with different strategies\n",
        "\n",
        "    Strategies:\n",
        "    - 'undersample': Randomly reduce majority class samples\n",
        "    - 'oversample': Repeat minority class samples\n",
        "    - 'hybrid': Combination of under and oversampling\n",
        "    \"\"\"\n",
        "    # Identify majority and minority classes\n",
        "    value_counts = y.value_counts()\n",
        "    majority_class = value_counts.index[0]\n",
        "    minority_classes = value_counts.index[1:]\n",
        "\n",
        "    if strategy == 'undersample':\n",
        "        # Undersample majority class\n",
        "        majority_samples = X[y == majority_class]\n",
        "        majority_labels = y[y == majority_class]\n",
        "\n",
        "        # Sample to match the next most frequent class\n",
        "        next_class_count = value_counts.iloc[1]\n",
        "        majority_downsampled = majority_samples.sample(\n",
        "            n=min(next_class_count * 2, len(majority_samples)),\n",
        "            random_state=42\n",
        "        )\n",
        "        majority_labels_downsampled = majority_labels.loc[majority_downsampled.index]\n",
        "\n",
        "        # Combine with other classes\n",
        "        X_balanced = pd.concat([\n",
        "            majority_downsampled,\n",
        "            X[y != majority_class]\n",
        "        ])\n",
        "        y_balanced = pd.concat([\n",
        "            majority_labels_downsampled,\n",
        "            y[y != majority_class]\n",
        "        ])\n",
        "\n",
        "    elif strategy == 'oversample':\n",
        "        # Oversample minority classes\n",
        "        max_class_count = value_counts.max()\n",
        "\n",
        "        oversampled_dfs = [X[y == majority_class]]\n",
        "        oversampled_labels = [y[y == majority_class]]\n",
        "\n",
        "        for minority_class in minority_classes:\n",
        "            minority_samples = X[y == minority_class]\n",
        "            minority_labels = y[y == minority_class]\n",
        "\n",
        "            # Oversample to match majority class\n",
        "            oversampled = minority_samples.sample(\n",
        "                n=max_class_count,\n",
        "                replace=True,\n",
        "                random_state=42\n",
        "            )\n",
        "            oversampled_labels_class = minority_labels.sample(\n",
        "                n=max_class_count,\n",
        "                replace=True,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            oversampled_dfs.append(oversampled)\n",
        "            oversampled_labels.append(oversampled_labels_class)\n",
        "\n",
        "        X_balanced = pd.concat(oversampled_dfs)\n",
        "        y_balanced = pd.concat(oversampled_labels)\n",
        "\n",
        "    elif strategy == 'hybrid':\n",
        "        # Combination of under and oversampling\n",
        "        max_class_count = value_counts.max()\n",
        "\n",
        "        balanced_dfs = []\n",
        "        balanced_labels = []\n",
        "\n",
        "        for unique_score in value_counts.index:\n",
        "            samples = X[y == unique_score]\n",
        "            labels = y[y == unique_score]\n",
        "\n",
        "            if len(samples) > max_class_count // 2:\n",
        "                # Undersample larger classes\n",
        "                samples_balanced = samples.sample(\n",
        "                    n=max_class_count // 2,\n",
        "                    random_state=42\n",
        "                )\n",
        "                labels_balanced = labels.loc[samples_balanced.index]\n",
        "            else:\n",
        "                # Oversample smaller classes\n",
        "                samples_balanced = samples.sample(\n",
        "                    n=max_class_count // 2,\n",
        "                    replace=True,\n",
        "                    random_state=42\n",
        "                )\n",
        "                labels_balanced = labels.sample(\n",
        "                    n=max_class_count // 2,\n",
        "                    replace=True,\n",
        "                    random_state=42\n",
        "                )\n",
        "\n",
        "            balanced_dfs.append(samples_balanced)\n",
        "            balanced_labels.append(labels_balanced)\n",
        "\n",
        "        X_balanced = pd.concat(balanced_dfs)\n",
        "        y_balanced = pd.concat(balanced_labels)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid strategy. Choose 'undersample', 'oversample', or 'hybrid'.\")\n",
        "\n",
        "    return X_balanced, y_balanced\n",
        "\n",
        "class XGBEnsembleRanker:\n",
        "    def __init__(self, n_models=5):\n",
        "        self.n_models = n_models\n",
        "        self.models = []\n",
        "        self.scalers = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i in range(self.n_models):\n",
        "            # Unique random state\n",
        "            np.random.seed(i*42)\n",
        "\n",
        "            # Scale features\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "            self.scalers.append(scaler)\n",
        "\n",
        "            # Create groups for ranking\n",
        "            unique_scores = np.unique(y)\n",
        "            group_sizes = [np.sum(y == score) for score in unique_scores]\n",
        "\n",
        "            model = XGBRanker(\n",
        "                objective='rank:pairwise',\n",
        "                learning_rate=0.05,\n",
        "                n_estimators=100,\n",
        "                max_depth=5,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=i*42\n",
        "            )\n",
        "\n",
        "            # Fit with groups\n",
        "            model.fit(\n",
        "                X_scaled,\n",
        "                y,\n",
        "                group=group_sizes,\n",
        "                verbose=False\n",
        "            )\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for scaler, model in zip(self.scalers, self.models):\n",
        "            X_scaled = scaler.transform(X)\n",
        "            pred = model.predict(X_scaled)\n",
        "            predictions.append(pred)\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "def run_evaluation(n_runs=10, balance_strategy='hybrid'):\n",
        "    all_metrics = {\n",
        "        'ndcg_5': [], 'ndcg_10': [], 'ndcg_20': [],\n",
        "        'spearman': [], 'perfect': [], 'close': []\n",
        "    }\n",
        "\n",
        "    # Print initial distribution\n",
        "    print(\"Original Score Distribution:\")\n",
        "    print(y.value_counts())\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "\n",
        "        # Handle imbalanced data\n",
        "        X_balanced, y_balanced = handle_imbalanced_data(X, y, strategy=balance_strategy)\n",
        "\n",
        "        # Print balanced distribution\n",
        "        print(\"Balanced Score Distribution:\")\n",
        "        print(y_balanced.value_counts())\n",
        "\n",
        "        # Split balanced data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_balanced, y_balanced, test_size=0.2, random_state=run*42\n",
        "        )\n",
        "\n",
        "        # Train ensemble and get predictions\n",
        "        ensemble = XGBEnsembleRanker(n_models=5)\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        predictions = ensemble.predict(X_test)\n",
        "\n",
        "        # Ensure predictions match test set length\n",
        "        predictions = predictions[:len(y_test)]\n",
        "\n",
        "        # Calculate NDCG\n",
        "        for k in [5, 10, 20]:\n",
        "            ndcg = ndcg_score(\n",
        "                y_true=y_test.values.reshape(1, -1),\n",
        "                y_score=predictions.reshape(1, -1),\n",
        "                k=k\n",
        "            )\n",
        "            all_metrics[f'ndcg_{k}'].append(ndcg)\n",
        "\n",
        "        # Detailed Ranking Analysis\n",
        "        results = pd.DataFrame({\n",
        "            'True_Score': y_test.reset_index(drop=True),\n",
        "            'Predicted_Score': pd.Series(predictions),\n",
        "            'True_Rank': y_test.reset_index(drop=True).rank(method='dense', ascending=False),\n",
        "            'Predicted_Rank': pd.Series(predictions).rank(method='dense', ascending=False)\n",
        "        })\n",
        "\n",
        "        # Compute Spearman correlation\n",
        "        spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n",
        "\n",
        "        test_size = len(y_test)  # Get test set size\n",
        "\n",
        "        # Count perfect and close matches\n",
        "        perfect = (results['True_Rank'] == results['Predicted_Rank']).sum()\n",
        "        close = (abs(results['True_Rank'] - results['Predicted_Rank']) <= 1).sum()\n",
        "\n",
        "        # Debugging prints\n",
        "        print(f\"Raw perfect count: {perfect}, Raw close count: {close}, Test size: {test_size}\")\n",
        "\n",
        "        # Normalize using 100 instead of 10\n",
        "        perfect = (perfect / test_size) * 100\n",
        "        close = (close / test_size) * 100\n",
        "\n",
        "        print(f\"Normalized perfect: {perfect}, Normalized close: {close}\")\n",
        "\n",
        "\n",
        "        all_metrics['spearman'].append(spearman_corr)\n",
        "        all_metrics['perfect'].append(perfect)\n",
        "        all_metrics['close'].append(close)\n",
        "\n",
        "    print(\"\\nAverage Performance:\")\n",
        "    print(\"=\" * 50)\n",
        "    for metric, values in all_metrics.items():\n",
        "        mean = np.mean(values)\n",
        "        std = np.std(values)\n",
        "        if 'ndcg' in metric:\n",
        "            print(f\"{metric.upper()}: {mean:.4f} ± {std:.4f}\")\n",
        "        elif metric == 'spearman':\n",
        "            print(f\"Spearman Correlation: {mean:.3f} ± {std:.3f}\")\n",
        "        else:\n",
        "            print(f\"{metric.title()}: {mean:.1f} ± {std:.1f} out of 10\")\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "# Run evaluation with different balancing strategies\n",
        "print(\"\\n--- Undersample Strategy ---\")\n",
        "metrics_undersample = run_evaluation(balance_strategy='undersample')\n",
        "\n",
        "print(\"\\n--- Oversample Strategy ---\")\n",
        "metrics_oversample = run_evaluation(balance_strategy='oversample')\n",
        "\n",
        "print(\"\\n--- Hybrid Strategy ---\")\n",
        "metrics_hybrid = run_evaluation(balance_strategy='hybrid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rHx9K6Zzhzjf",
        "outputId": "1a5a696c-fb68-499c-a4e4-f26052ea9697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Undersample Strategy ---\n",
            "Original Score Distribution:\n",
            "reason_score_100\n",
            "100.0    5097\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 1/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 0, Raw close count: 1, Test size: 1724\n",
            "Normalized perfect: 0.0, Normalized close: 0.058004640371229696\n",
            "\n",
            "Run 2/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 1, Raw close count: 5, Test size: 1724\n",
            "Normalized perfect: 0.058004640371229696, Normalized close: 0.2900232018561485\n",
            "\n",
            "Run 3/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 1, Raw close count: 4, Test size: 1724\n",
            "Normalized perfect: 0.058004640371229696, Normalized close: 0.23201856148491878\n",
            "\n",
            "Run 4/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 3, Raw close count: 3, Test size: 1724\n",
            "Normalized perfect: 0.17401392111368907, Normalized close: 0.17401392111368907\n",
            "\n",
            "Run 5/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 2, Raw close count: 2, Test size: 1724\n",
            "Normalized perfect: 0.11600928074245939, Normalized close: 0.11600928074245939\n",
            "\n",
            "Run 6/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 3, Raw close count: 4, Test size: 1724\n",
            "Normalized perfect: 0.17401392111368907, Normalized close: 0.23201856148491878\n",
            "\n",
            "Run 7/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 0, Raw close count: 2, Test size: 1724\n",
            "Normalized perfect: 0.0, Normalized close: 0.11600928074245939\n",
            "\n",
            "Run 8/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 5, Raw close count: 9, Test size: 1724\n",
            "Normalized perfect: 0.2900232018561485, Normalized close: 0.5220417633410672\n",
            "\n",
            "Run 9/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 0, Raw close count: 6, Test size: 1724\n",
            "Normalized perfect: 0.0, Normalized close: 0.34802784222737815\n",
            "\n",
            "Run 10/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 1, Raw close count: 2, Test size: 1724\n",
            "Normalized perfect: 0.058004640371229696, Normalized close: 0.11600928074245939\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.7705 ± 0.1372\n",
            "NDCG_10: 0.7795 ± 0.0828\n",
            "NDCG_20: 0.7843 ± 0.0625\n",
            "Spearman Correlation: 0.162 ± 0.015\n",
            "Perfect: 0.1 ± 0.1 out of 10\n",
            "Close: 0.2 ± 0.1 out of 10\n",
            "\n",
            "--- Oversample Strategy ---\n",
            "Original Score Distribution:\n",
            "reason_score_100\n",
            "100.0    5097\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 1/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 25, Raw close count: 91, Test size: 69320\n",
            "Normalized perfect: 0.03606462781304097, Normalized close: 0.13127524523946912\n",
            "\n",
            "Run 2/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 33, Raw close count: 56, Test size: 69320\n",
            "Normalized perfect: 0.04760530871321408, Normalized close: 0.08078476630121177\n",
            "\n",
            "Run 3/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "Raw perfect count: 50, Raw close count: 175, Test size: 69320\n",
            "Normalized perfect: 0.07212925562608194, Normalized close: 0.2524523946912868\n",
            "\n",
            "Run 4/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-aad5e54c091f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Oversample Strategy ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m \u001b[0mmetrics_oversample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalance_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'oversample'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Hybrid Strategy ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-aad5e54c091f>\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(n_runs, balance_strategy)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# Train ensemble and get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBEnsembleRanker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-aad5e54c091f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# Fit with groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             model.fit(\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, group, qid, sample_weight, base_margin, eval_set, eval_group, eval_qid, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   2019\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   2022\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRanker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Clean numeric features\n",
        "    numeric_features = ['Impact_Factor', 'original_altmetric_score', 'original_cited_by_posts_count']\n",
        "    for col in numeric_features:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Create ratios and interactions with epsilon to avoid division by zero\n",
        "    df['if_alt_ratio'] = df['Impact_Factor'] / (df['original_altmetric_score'] + 1e-5)\n",
        "    df['if_cit_ratio'] = df['Impact_Factor'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "    df['alt_cit_ratio'] = df['original_altmetric_score'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Log transformations with epsilon\n",
        "    df['log_impact_factor'] = np.log1p(df['Impact_Factor'] + 1e-5)\n",
        "    df['log_altmetric'] = np.log1p(df['original_altmetric_score'] + 1e-5)\n",
        "    df['log_citations'] = np.log1p(df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Create percentile ranks\n",
        "    df['if_rank'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['alt_rank'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['cit_rank'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Create composite scores\n",
        "    df['impact_score'] = (df['if_rank'] + df['alt_rank'] + df['cit_rank']) / 3\n",
        "    df['weighted_impact'] = (df['if_rank'] * 0.4 + df['alt_rank'] * 0.3 + df['cit_rank'] * 0.3)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Convert scores to 1-100 scale\n",
        "df['reason_score_100'] = (df['reason_score'] * 10).clip(1, 100)\n",
        "\n",
        "# Apply feature engineering\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "# Define features\n",
        "features = [\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count',\n",
        "    'if_alt_ratio',\n",
        "    'if_cit_ratio',\n",
        "    'alt_cit_ratio',\n",
        "    'log_impact_factor',\n",
        "    'log_altmetric',\n",
        "    'log_citations',\n",
        "    'if_rank',\n",
        "    'alt_rank',\n",
        "    'cit_rank',\n",
        "    'impact_score',\n",
        "    'weighted_impact'\n",
        "]\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_engineered[features]\n",
        "y = df_engineered['reason_score_100']\n",
        "\n",
        "def handle_imbalanced_data(X, y, strategy='hybrid'):\n",
        "    \"\"\"\n",
        "    Handle imbalanced dataset with different strategies\n",
        "\n",
        "    Strategies:\n",
        "    - 'undersample': Randomly reduce majority class samples\n",
        "    - 'oversample': Repeat minority class samples\n",
        "    - 'hybrid': Combination of under and oversampling\n",
        "    \"\"\"\n",
        "    # Identify majority and minority classes\n",
        "    value_counts = y.value_counts()\n",
        "    majority_class = value_counts.index[0]\n",
        "    minority_classes = value_counts.index[1:]\n",
        "\n",
        "    if strategy == 'undersample':\n",
        "        # Undersample majority class\n",
        "        majority_samples = X[y == majority_class]\n",
        "        majority_labels = y[y == majority_class]\n",
        "\n",
        "        # Sample to match the next most frequent class\n",
        "        next_class_count = value_counts.iloc[1]\n",
        "        majority_downsampled = majority_samples.sample(\n",
        "            n=min(next_class_count * 2, len(majority_samples)),\n",
        "            random_state=42\n",
        "        )\n",
        "        majority_labels_downsampled = majority_labels.loc[majority_downsampled.index]\n",
        "\n",
        "        # Combine with other classes\n",
        "        X_balanced = pd.concat([\n",
        "            majority_downsampled,\n",
        "            X[y != majority_class]\n",
        "        ])\n",
        "        y_balanced = pd.concat([\n",
        "            majority_labels_downsampled,\n",
        "            y[y != majority_class]\n",
        "        ])\n",
        "\n",
        "    elif strategy == 'oversample':\n",
        "        # Oversample minority classes\n",
        "        max_class_count = value_counts.max()\n",
        "\n",
        "        oversampled_dfs = [X[y == majority_class]]\n",
        "        oversampled_labels = [y[y == majority_class]]\n",
        "\n",
        "        for minority_class in minority_classes:\n",
        "            minority_samples = X[y == minority_class]\n",
        "            minority_labels = y[y == minority_class]\n",
        "\n",
        "            # Oversample to match majority class\n",
        "            oversampled = minority_samples.sample(\n",
        "                n=max_class_count,\n",
        "                replace=True,\n",
        "                random_state=42\n",
        "            )\n",
        "            oversampled_labels_class = minority_labels.sample(\n",
        "                n=max_class_count,\n",
        "                replace=True,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            oversampled_dfs.append(oversampled)\n",
        "            oversampled_labels.append(oversampled_labels_class)\n",
        "\n",
        "        X_balanced = pd.concat(oversampled_dfs)\n",
        "        y_balanced = pd.concat(oversampled_labels)\n",
        "\n",
        "    elif strategy == 'hybrid':\n",
        "        # Combination of under and oversampling\n",
        "        max_class_count = value_counts.max()\n",
        "\n",
        "        balanced_dfs = []\n",
        "        balanced_labels = []\n",
        "\n",
        "        for unique_score in value_counts.index:\n",
        "            samples = X[y == unique_score]\n",
        "            labels = y[y == unique_score]\n",
        "\n",
        "            if len(samples) > max_class_count // 2:\n",
        "                # Undersample larger classes\n",
        "                samples_balanced = samples.sample(\n",
        "                    n=max_class_count // 2,\n",
        "                    random_state=42\n",
        "                )\n",
        "                labels_balanced = labels.loc[samples_balanced.index]\n",
        "            else:\n",
        "                # Oversample smaller classes\n",
        "                samples_balanced = samples.sample(\n",
        "                    n=max_class_count // 2,\n",
        "                    replace=True,\n",
        "                    random_state=42\n",
        "                )\n",
        "                labels_balanced = labels.sample(\n",
        "                    n=max_class_count // 2,\n",
        "                    replace=True,\n",
        "                    random_state=42\n",
        "                )\n",
        "\n",
        "            balanced_dfs.append(samples_balanced)\n",
        "            balanced_labels.append(labels_balanced)\n",
        "\n",
        "        X_balanced = pd.concat(balanced_dfs)\n",
        "        y_balanced = pd.concat(balanced_labels)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid strategy. Choose 'undersample', 'oversample', or 'hybrid'.\")\n",
        "\n",
        "    return X_balanced, y_balanced\n",
        "\n",
        "class XGBEnsembleRanker:\n",
        "    def __init__(self, n_models=5):\n",
        "        self.n_models = n_models\n",
        "        self.models = []\n",
        "        self.scalers = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i in range(self.n_models):\n",
        "            # Unique random state\n",
        "            np.random.seed(i*42)\n",
        "\n",
        "            # Scale features\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "            self.scalers.append(scaler)\n",
        "\n",
        "            # Create groups for ranking\n",
        "            unique_scores = np.unique(y)\n",
        "            group_sizes = [np.sum(y == score) for score in unique_scores]\n",
        "\n",
        "            model = XGBRanker(\n",
        "                objective='rank:pairwise',\n",
        "                learning_rate=0.05,\n",
        "                n_estimators=100,\n",
        "                max_depth=5,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=i*42\n",
        "            )\n",
        "\n",
        "            # Fit with groups\n",
        "            model.fit(\n",
        "                X_scaled,\n",
        "                y,\n",
        "                group=group_sizes,\n",
        "                verbose=False\n",
        "            )\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for scaler, model in zip(self.scalers, self.models):\n",
        "            X_scaled = scaler.transform(X)\n",
        "            pred = model.predict(X_scaled)\n",
        "            predictions.append(pred)\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "def run_evaluation(n_runs=10, balance_strategy='hybrid'):\n",
        "    all_metrics = {\n",
        "        'ndcg_5': [], 'ndcg_10': [], 'ndcg_20': [],\n",
        "        'spearman': [], 'perfect': [], 'close': []\n",
        "    }\n",
        "\n",
        "    # Print initial distribution\n",
        "    print(\"Original Score Distribution:\")\n",
        "    print(y.value_counts())\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "\n",
        "        # Handle imbalanced data\n",
        "        X_balanced, y_balanced = handle_imbalanced_data(X, y, strategy=balance_strategy)\n",
        "\n",
        "        # Print balanced distribution\n",
        "        print(\"Balanced Score Distribution:\")\n",
        "        print(y_balanced.value_counts())\n",
        "\n",
        "        # Split balanced data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_balanced, y_balanced, test_size=0.2, random_state=run*42\n",
        "        )\n",
        "\n",
        "        # Train ensemble and get predictions\n",
        "        ensemble = XGBEnsembleRanker(n_models=5)\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        predictions = ensemble.predict(X_test)\n",
        "\n",
        "        # Ensure predictions match test set length\n",
        "        predictions = predictions[:len(y_test)]\n",
        "\n",
        "        # Detailed Ranking Analysis\n",
        "        results = pd.DataFrame({\n",
        "            'True_Score': y_test.reset_index(drop=True),\n",
        "            'Predicted_Score': pd.Series(predictions),\n",
        "            'True_Rank': y_test.reset_index(drop=True).rank(method='dense', ascending=False),  # Reverse the rank order\n",
        "            'Predicted_Rank': pd.Series(predictions).rank(method='dense', ascending=False)  # Reverse the rank order\n",
        "        })\n",
        "\n",
        "        # Compute Spearman correlation\n",
        "        spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n",
        "\n",
        "        # Calculate Perfect and Close Matches\n",
        "        perfect = (results['True_Rank'] == results['Predicted_Rank']).sum()\n",
        "        close = (abs(results['True_Rank'] - results['Predicted_Rank']) <= 1).sum()\n",
        "\n",
        "        # Normalize to a scale of 10\n",
        "        test_size = len(y_test)  # Get the actual number of test samples\n",
        "        perfect = (perfect / test_size) * 10\n",
        "        close = (close / test_size) * 10\n",
        "\n",
        "        all_metrics['spearman'].append(spearman_corr)\n",
        "        all_metrics['perfect'].append(perfect)\n",
        "        all_metrics['close'].append(close)\n",
        "\n",
        "        # Calculate NDCG for different values of k\n",
        "        for k in [5, 10, 20]:\n",
        "            ndcg = ndcg_score(\n",
        "                y_true=y_test.values.reshape(1, -1),\n",
        "                y_score=predictions.reshape(1, -1),\n",
        "                k=k\n",
        "            )\n",
        "            all_metrics[f'ndcg_{k}'].append(ndcg)\n",
        "\n",
        "    print(\"\\nAverage Performance:\")\n",
        "    print(\"=\" * 50)\n",
        "    for metric, values in all_metrics.items():\n",
        "        mean = np.mean(values)\n",
        "        std = np.std(values)\n",
        "        if 'ndcg' in metric:\n",
        "            print(f\"{metric.upper()}: {mean:.4f} ± {std:.4f}\")\n",
        "        elif metric == 'spearman':\n",
        "            print(f\"Spearman Correlation: {mean:.3f} ± {std:.3f}\")\n",
        "        else:\n",
        "            print(f\"{metric.title()}: {mean:.1f} ± {std:.1f} out of 10\")\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "# Run evaluation with different balancing strategies\n",
        "print(\"\\n--- Undersample Strategy ---\")\n",
        "metrics_undersample = run_evaluation(balance_strategy='undersample')\n",
        "\n",
        "print(\"\\n--- Oversample Strategy ---\")\n",
        "metrics_oversample = run_evaluation(balance_strategy='oversample')\n",
        "\n",
        "print(\"\\n--- Hybrid Strategy ---\")\n",
        "metrics_hybrid = run_evaluation(balance_strategy='hybrid')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX7qktSGmgrd",
        "outputId": "a24d5779-bace-4660-b464-2ff62ba749db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Undersample Strategy ---\n",
            "Original Score Distribution:\n",
            "reason_score_100\n",
            "100.0    5097\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 1/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 2/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 3/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 4/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 5/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 6/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 7/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 8/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 9/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 10/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "100.0    1522\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.7705 ± 0.1372\n",
            "NDCG_10: 0.7795 ± 0.0828\n",
            "NDCG_20: 0.7843 ± 0.0625\n",
            "Spearman Correlation: 0.162 ± 0.015\n",
            "Perfect: 0.0 ± 0.0 out of 10\n",
            "Close: 0.0 ± 0.0 out of 10\n",
            "\n",
            "--- Oversample Strategy ---\n",
            "Original Score Distribution:\n",
            "reason_score_100\n",
            "100.0    5097\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 1/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 2/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 3/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 4/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 5/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 6/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 7/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 8/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 9/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 10/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     5097\n",
            "100.0    5097\n",
            "40.0     5097\n",
            "65.0     5097\n",
            "32.0     5097\n",
            "         ... \n",
            "30.0     5097\n",
            "95.0     5097\n",
            "88.0     5097\n",
            "83.0     5097\n",
            "93.0     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.9506 ± 0.0064\n",
            "NDCG_10: 0.9506 ± 0.0064\n",
            "NDCG_20: 0.9506 ± 0.0064\n",
            "Spearman Correlation: 0.338 ± 0.010\n",
            "Perfect: 0.0 ± 0.0 out of 10\n",
            "Close: 0.0 ± 0.0 out of 10\n",
            "\n",
            "--- Hybrid Strategy ---\n",
            "Original Score Distribution:\n",
            "reason_score_100\n",
            "100.0    5097\n",
            "40.0      761\n",
            "65.0      713\n",
            "73.0      460\n",
            "48.0      408\n",
            "         ... \n",
            "62.0        3\n",
            "17.0        3\n",
            "37.0        2\n",
            "52.0        1\n",
            "18.0        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 1/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 2/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 3/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 4/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 5/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 6/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 7/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 8/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 9/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 10/10\n",
            "Balanced Score Distribution:\n",
            "reason_score_100\n",
            "18.0     2548\n",
            "100.0    2548\n",
            "40.0     2548\n",
            "65.0     2548\n",
            "32.0     2548\n",
            "         ... \n",
            "30.0     2548\n",
            "95.0     2548\n",
            "88.0     2548\n",
            "83.0     2548\n",
            "93.0     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.9453 ± 0.0125\n",
            "NDCG_10: 0.9453 ± 0.0125\n",
            "NDCG_20: 0.9453 ± 0.0125\n",
            "Spearman Correlation: 0.344 ± 0.011\n",
            "Perfect: 0.0 ± 0.0 out of 10\n",
            "Close: 0.0 ± 0.0 out of 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRanker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "# Feature engineering\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "    # Example of numeric and non-numeric feature engineering:\n",
        "    df['if_alt_ratio'] = df['Impact_Factor'] / (df['original_altmetric_score'] + 1e-5)\n",
        "    df['if_cit_ratio'] = df['Impact_Factor'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "    df['log_impact_factor'] = np.log1p(df['Impact_Factor'] + 1e-5)\n",
        "    df['log_altmetric'] = np.log1p(df['original_altmetric_score'] + 1e-5)\n",
        "    df['log_citations'] = np.log1p(df['original_cited_by_posts_count'] + 1e-5)\n",
        "    df['impact_score'] = (df['if_alt_ratio'] + df['if_cit_ratio'] + df['log_impact_factor']) / 3\n",
        "    return df\n",
        "\n",
        "df = engineer_features(df)\n",
        "\n",
        "# Define features\n",
        "features = ['Impact_Factor', 'original_altmetric_score', 'original_cited_by_posts_count',\n",
        "            'if_alt_ratio', 'if_cit_ratio', 'log_impact_factor', 'log_altmetric',\n",
        "            'log_citations', 'impact_score']\n",
        "\n",
        "X = df[features]\n",
        "y = df['reason_score_100']  # assuming the target is the trustworthiness score\n",
        "\n",
        "# Training the model\n",
        "class XGBEnsembleRanker:\n",
        "    def __init__(self, n_models=5):\n",
        "        self.n_models = n_models\n",
        "        self.models = []\n",
        "        self.scalers = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i in range(self.n_models):\n",
        "            np.random.seed(i*42)\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "            self.scalers.append(scaler)\n",
        "\n",
        "            unique_scores = np.unique(y)\n",
        "            group_sizes = [np.sum(y == score) for score in unique_scores]\n",
        "\n",
        "            model = XGBRanker(\n",
        "                objective='rank:pairwise',\n",
        "                learning_rate=0.05,\n",
        "                n_estimators=100,\n",
        "                max_depth=5,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=i*42\n",
        "            )\n",
        "            model.fit(X_scaled, y, group=group_sizes, verbose=False)\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for scaler, model in zip(self.scalers, self.models):\n",
        "            X_scaled = scaler.transform(X)\n",
        "            pred = model.predict(X_scaled)\n",
        "            predictions.append(pred)\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "# Run evaluation with 10 documents\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "ensemble = XGBEnsembleRanker(n_models=5)\n",
        "ensemble.fit(X_train, y_train)\n",
        "predictions = ensemble.predict(X_test)\n",
        "\n",
        "# Evaluate ranking performance\n",
        "ndcg_at_5 = ndcg_score(y_test.values.reshape(1, -1), predictions.reshape(1, -1), k=5)\n",
        "\n",
        "print(f\"NDCG@5: {ndcg_at_5:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "KDubiAIR4d0j",
        "outputId": "f4a1d714-bda4-48f1-f154-6fa987077eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for /: 'str' and 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;31m# error: \"None\" not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'float'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1bf176e639b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengineer_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Define features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-1bf176e639b0>\u001b[0m in \u001b[0;36mengineer_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Example of numeric and non-numeric feature engineering:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'if_alt_ratio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Impact_Factor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original_altmetric_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'if_cit_ratio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Impact_Factor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original_cited_by_posts_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_impact_factor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Impact_Factor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__truediv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__truediv__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__truediv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rtruediv__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_align_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexOpsMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_align_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_asobject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Don't do this for comparisons, as that will handle complex numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;31m#  incorrectly, see GH#32047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_masked_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_masked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myrav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRanker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Clean numeric features\n",
        "    numeric_features = ['Impact_Factor', 'original_altmetric_score', 'original_cited_by_posts_count']\n",
        "    for col in numeric_features:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Create ratios and interactions with epsilon to avoid division by zero\n",
        "    df['if_alt_ratio'] = df['Impact_Factor'] / (df['original_altmetric_score'] + 1e-5)\n",
        "    df['if_cit_ratio'] = df['Impact_Factor'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "    df['alt_cit_ratio'] = df['original_altmetric_score'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Log transformations with epsilon\n",
        "    df['log_impact_factor'] = np.log1p(df['Impact_Factor'] + 1e-5)\n",
        "    df['log_altmetric'] = np.log1p(df['original_altmetric_score'] + 1e-5)\n",
        "    df['log_citations'] = np.log1p(df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Create percentile ranks\n",
        "    df['if_rank'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['alt_rank'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['cit_rank'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Create composite scores\n",
        "    df['impact_score'] = (df['if_rank'] + df['alt_rank'] + df['cit_rank']) / 3\n",
        "    df['weighted_impact'] = (df['if_rank'] * 0.4 + df['alt_rank'] * 0.3 + df['cit_rank'] * 0.3)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Convert scores to 1-100 scale if needed\n",
        "# df['reason_score_100'] = (df['reason_score'] * 10).clip(1, 100)  # Uncomment if scaling is required\n",
        "\n",
        "# Apply feature engineering\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "# Define features\n",
        "features = [\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count',\n",
        "    'if_alt_ratio',\n",
        "    'if_cit_ratio',\n",
        "    'alt_cit_ratio',\n",
        "    'log_impact_factor',\n",
        "    'log_altmetric',\n",
        "    'log_citations',\n",
        "    'if_rank',\n",
        "    'alt_rank',\n",
        "    'cit_rank',\n",
        "    'impact_score',\n",
        "    'weighted_impact'\n",
        "]\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_engineered[features]\n",
        "y = df_engineered['reason_score']  # Use 'reason_score' instead of 'reason_score_100'\n",
        "\n",
        "def handle_imbalanced_data(X, y, strategy='hybrid'):\n",
        "    \"\"\"\n",
        "    Handle imbalanced dataset with different strategies\n",
        "\n",
        "    Strategies:\n",
        "    - 'undersample': Randomly reduce majority class samples\n",
        "    - 'oversample': Repeat minority class samples\n",
        "    - 'hybrid': Combination of under and oversampling\n",
        "    \"\"\"\n",
        "    # Identify majority and minority classes\n",
        "    value_counts = y.value_counts()\n",
        "    majority_class = value_counts.index[0]\n",
        "    minority_classes = value_counts.index[1:]\n",
        "\n",
        "    if strategy == 'undersample':\n",
        "        # Undersample majority class\n",
        "        majority_samples = X[y == majority_class]\n",
        "        majority_labels = y[y == majority_class]\n",
        "\n",
        "        # Sample to match the next most frequent class\n",
        "        next_class_count = value_counts.iloc[1]\n",
        "        majority_downsampled = majority_samples.sample(\n",
        "            n=min(next_class_count * 2, len(majority_samples)),\n",
        "            random_state=42\n",
        "        )\n",
        "        majority_labels_downsampled = majority_labels.loc[majority_downsampled.index]\n",
        "\n",
        "        # Combine with other classes\n",
        "        X_balanced = pd.concat([\n",
        "            majority_downsampled,\n",
        "            X[y != majority_class]\n",
        "        ])\n",
        "        y_balanced = pd.concat([\n",
        "            majority_labels_downsampled,\n",
        "            y[y != majority_class]\n",
        "        ])\n",
        "\n",
        "    elif strategy == 'oversample':\n",
        "        # Oversample minority classes\n",
        "        max_class_count = value_counts.max()\n",
        "\n",
        "        oversampled_dfs = [X[y == majority_class]]\n",
        "        oversampled_labels = [y[y == majority_class]]\n",
        "\n",
        "        for minority_class in minority_classes:\n",
        "            minority_samples = X[y == minority_class]\n",
        "            minority_labels = y[y == minority_class]\n",
        "\n",
        "            # Oversample to match majority class\n",
        "            oversampled = minority_samples.sample(\n",
        "                n=max_class_count,\n",
        "                replace=True,\n",
        "                random_state=42\n",
        "            )\n",
        "            oversampled_labels_class = minority_labels.sample(\n",
        "                n=max_class_count,\n",
        "                replace=True,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            oversampled_dfs.append(oversampled)\n",
        "            oversampled_labels.append(oversampled_labels_class)\n",
        "\n",
        "        X_balanced = pd.concat(oversampled_dfs)\n",
        "        y_balanced = pd.concat(oversampled_labels)\n",
        "\n",
        "    elif strategy == 'hybrid':\n",
        "        # Combination of under and oversampling\n",
        "        max_class_count = value_counts.max()\n",
        "\n",
        "        balanced_dfs = []\n",
        "        balanced_labels = []\n",
        "\n",
        "        for unique_score in value_counts.index:\n",
        "            samples = X[y == unique_score]\n",
        "            labels = y[y == unique_score]\n",
        "\n",
        "            if len(samples) > max_class_count // 2:\n",
        "                # Undersample larger classes\n",
        "                samples_balanced = samples.sample(\n",
        "                    n=max_class_count // 2,\n",
        "                    random_state=42\n",
        "                )\n",
        "                labels_balanced = labels.loc[samples_balanced.index]\n",
        "            else:\n",
        "                # Oversample smaller classes\n",
        "                samples_balanced = samples.sample(\n",
        "                    n=max_class_count // 2,\n",
        "                    replace=True,\n",
        "                    random_state=42\n",
        "                )\n",
        "                labels_balanced = labels.sample(\n",
        "                    n=max_class_count // 2,\n",
        "                    replace=True,\n",
        "                    random_state=42\n",
        "                )\n",
        "\n",
        "            balanced_dfs.append(samples_balanced)\n",
        "            balanced_labels.append(labels_balanced)\n",
        "\n",
        "        X_balanced = pd.concat(balanced_dfs)\n",
        "        y_balanced = pd.concat(balanced_labels)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid strategy. Choose 'undersample', 'oversample', or 'hybrid'.\")\n",
        "\n",
        "    return X_balanced, y_balanced\n",
        "\n",
        "class XGBEnsembleRanker:\n",
        "    def __init__(self, n_models=5):\n",
        "        self.n_models = n_models\n",
        "        self.models = []\n",
        "        self.scalers = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i in range(self.n_models):\n",
        "            # Unique random state\n",
        "            np.random.seed(i*42)\n",
        "\n",
        "            # Scale features\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "            self.scalers.append(scaler)\n",
        "\n",
        "            # Create groups for ranking\n",
        "            unique_scores = np.unique(y)\n",
        "            group_sizes = [np.sum(y == score) for score in unique_scores]\n",
        "\n",
        "            model = XGBRanker(\n",
        "                objective='rank:pairwise',\n",
        "                learning_rate=0.05,\n",
        "                n_estimators=100,\n",
        "                max_depth=5,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=i*42\n",
        "            )\n",
        "\n",
        "            # Fit with groups\n",
        "            model.fit(\n",
        "                X_scaled,\n",
        "                y,\n",
        "                group=group_sizes,\n",
        "                verbose=False\n",
        "            )\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for scaler, model in zip(self.scalers, self.models):\n",
        "            X_scaled = scaler.transform(X)\n",
        "            pred = model.predict(X_scaled)\n",
        "            predictions.append(pred)\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "def run_evaluation(n_runs=10, balance_strategy='hybrid'):\n",
        "    all_metrics = {\n",
        "        'ndcg_5': [], 'ndcg_10': [], 'ndcg_20': [],\n",
        "        'spearman': [], 'perfect': [], 'close': []\n",
        "    }\n",
        "\n",
        "    # Print initial distribution\n",
        "    print(\"Original Score Distribution:\")\n",
        "    print(y.value_counts())\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "\n",
        "        # Handle imbalanced data\n",
        "        X_balanced, y_balanced = handle_imbalanced_data(X, y, strategy=balance_strategy)\n",
        "\n",
        "        # Print balanced distribution\n",
        "        print(\"Balanced Score Distribution:\")\n",
        "        print(y_balanced.value_counts())\n",
        "\n",
        "        # Split balanced data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_balanced, y_balanced, test_size=0.2, random_state=run*42\n",
        "        )\n",
        "\n",
        "        # Train ensemble and get predictions\n",
        "        ensemble = XGBEnsembleRanker(n_models=5)\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        predictions = ensemble.predict(X_test)\n",
        "\n",
        "        # Ensure predictions match test set length\n",
        "        predictions = predictions[:len(y_test)]\n",
        "\n",
        "        # Calculate NDCG\n",
        "        for k in [5, 10, 20]:\n",
        "            ndcg = ndcg_score(\n",
        "                y_true=y_test.values.reshape(1, -1),\n",
        "                y_score=predictions.reshape(1, -1),\n",
        "                k=k\n",
        "            )\n",
        "            all_metrics[f'ndcg_{k}'].append(ndcg)\n",
        "\n",
        "        # Detailed Ranking Analysis\n",
        "        results = pd.DataFrame({\n",
        "            'True_Score': y_test.reset_index(drop=True),\n",
        "            'Predicted_Score': pd.Series(predictions),\n",
        "            'True_Rank': y_test.reset_index(drop=True).rank(method='dense', ascending=False),\n",
        "            'Predicted_Rank': pd.Series(predictions).rank(method='dense', ascending=False)\n",
        "        })\n",
        "\n",
        "        # Compute Spearman correlation\n",
        "        spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n",
        "\n",
        "        perfect = (results['True_Rank'] == results['Predicted_Rank']).sum()\n",
        "        close = (abs(results['True_Rank'] - results['Predicted_Rank']) <= 1).sum()\n",
        "\n",
        "        all_metrics['spearman'].append(spearman_corr)\n",
        "        all_metrics['perfect'].append(perfect)\n",
        "        all_metrics['close'].append(close)\n",
        "\n",
        "    print(\"\\nAverage Performance:\")\n",
        "    print(\"=\" * 50)\n",
        "    for metric, values in all_metrics.items():\n",
        "        mean = np.mean(values)\n",
        "        std = np.std(values)\n",
        "        if 'ndcg' in metric:\n",
        "            print(f\"{metric.upper()}: {mean:.4f} ± {std:.4f}\")\n",
        "        elif metric == 'spearman':\n",
        "            print(f\"Spearman Correlation: {mean:.3f} ± {std:.3f}\")\n",
        "        else:\n",
        "            print(f\"{metric.title()}: {mean:.1f} ± {std:.1f} out of 10\")\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "# Run evaluation with different balancing strategies\n",
        "print(\"\\n--- Undersample Strategy ---\")\n",
        "metrics_undersample = run_evaluation(balance_strategy='undersample')\n",
        "\n",
        "print(\"\\n--- Oversample Strategy ---\")\n",
        "metrics_oversample = run_evaluation(balance_strategy='oversample')\n",
        "\n",
        "print(\"\\n--- Hybrid Strategy ---\")\n",
        "metrics_hybrid = run_evaluation(balance_strategy='hybrid')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLSlklWm6PXD",
        "outputId": "1f6fa5c7-3c4f-4fe8-aee3-15239a5ecc0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Undersample Strategy ---\n",
            "Original Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 1/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 2/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 3/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 4/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 5/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 6/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 7/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 8/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 9/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 10/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    1522\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.7705 ± 0.1372\n",
            "NDCG_10: 0.7795 ± 0.0828\n",
            "NDCG_20: 0.7843 ± 0.0625\n",
            "Spearman Correlation: 0.162 ± 0.015\n",
            "Perfect: 1.6 ± 1.6 out of 10\n",
            "Close: 3.8 ± 2.3 out of 10\n",
            "\n",
            "--- Oversample Strategy ---\n",
            "Original Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 1/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 2/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 3/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 4/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 5/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 6/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 7/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 8/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 9/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 10/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     5097\n",
            "10.0    5097\n",
            "4.0     5097\n",
            "6.5     5097\n",
            "3.2     5097\n",
            "        ... \n",
            "3.0     5097\n",
            "9.5     5097\n",
            "8.8     5097\n",
            "8.3     5097\n",
            "9.3     5097\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.9506 ± 0.0064\n",
            "NDCG_10: 0.9506 ± 0.0064\n",
            "NDCG_20: 0.9506 ± 0.0064\n",
            "Spearman Correlation: 0.338 ± 0.010\n",
            "Perfect: 27.4 ± 14.5 out of 10\n",
            "Close: 90.6 ± 36.2 out of 10\n",
            "\n",
            "--- Hybrid Strategy ---\n",
            "Original Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      761\n",
            "6.5      713\n",
            "7.3      460\n",
            "4.8      408\n",
            "        ... \n",
            "6.2        3\n",
            "1.7        3\n",
            "3.7        2\n",
            "5.2        1\n",
            "1.8        1\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 1/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 2/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 3/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 4/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 5/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 6/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 7/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 8/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 9/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Run 10/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "1.8     2548\n",
            "10.0    2548\n",
            "4.0     2548\n",
            "6.5     2548\n",
            "3.2     2548\n",
            "        ... \n",
            "3.0     2548\n",
            "9.5     2548\n",
            "8.8     2548\n",
            "8.3     2548\n",
            "9.3     2548\n",
            "Name: count, Length: 68, dtype: int64\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.9453 ± 0.0125\n",
            "NDCG_10: 0.9453 ± 0.0125\n",
            "NDCG_20: 0.9453 ± 0.0125\n",
            "Spearman Correlation: 0.344 ± 0.011\n",
            "Perfect: 21.6 ± 15.5 out of 10\n",
            "Close: 63.5 ± 41.3 out of 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.utils import resample\n",
        "from xgboost import XGBRanker\n",
        "from sklearn.metrics import ndcg_score\n",
        "from scipy.stats import spearmanr  # Corrected import\n",
        "\n",
        "# Function to handle imbalanced data (oversampling)\n",
        "def handle_imbalanced_data(X, y, strategy='oversample'):\n",
        "    if strategy == 'oversample':\n",
        "        # Combine X and y for resampling\n",
        "        data = pd.concat([X, y], axis=1)\n",
        "        # Separate each class\n",
        "        majority_class = data[data[y.name] == data[y.name].mode()[0]]\n",
        "        minority_class = data[data[y.name] != data[y.name].mode()[0]]\n",
        "\n",
        "        # Oversample minority class\n",
        "        minority_upsampled = resample(minority_class,\n",
        "                                      replace=True,  # sample with replacement\n",
        "                                      n_samples=majority_class.shape[0],  # match majority class size\n",
        "                                      random_state=42)\n",
        "\n",
        "        # Combine majority and upsampled minority class\n",
        "        upsampled_data = pd.concat([majority_class, minority_upsampled])\n",
        "        X_upsampled = upsampled_data.drop(y.name, axis=1)\n",
        "        y_upsampled = upsampled_data[y.name]\n",
        "\n",
        "        return X_upsampled, y_upsampled\n",
        "    else:\n",
        "        return X, y  # No resampling if strategy is not 'oversample'\n",
        "\n",
        "# Function to evaluate the model with hyperparameter tuning and oversampling\n",
        "def run_evaluation_with_tuning(n_runs=10, balance_strategy='oversample'):\n",
        "    all_metrics = {\n",
        "        'ndcg_5': [], 'ndcg_10': [], 'ndcg_20': [],\n",
        "        'spearman': [], 'perfect': [], 'close': []\n",
        "    }\n",
        "\n",
        "    # Define hyperparameter grid for tuning\n",
        "    param_grid = {\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.7, 0.8, 0.9],\n",
        "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "\n",
        "        # Handle imbalanced data with oversampling\n",
        "        X_balanced, y_balanced = handle_imbalanced_data(X, y, strategy=balance_strategy)\n",
        "\n",
        "        # Create group column: Since we're treating all samples as part of the same group\n",
        "        group = np.ones(len(X_balanced))  # This assigns the same group to all data points\n",
        "\n",
        "        # Print balanced distribution\n",
        "        print(\"Balanced Score Distribution:\")\n",
        "        print(y_balanced.value_counts())\n",
        "\n",
        "        # Split balanced data\n",
        "        X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
        "            X_balanced, y_balanced, group, test_size=0.2, random_state=run*42\n",
        "        )\n",
        "\n",
        "        # Initialize the XGBRanker model\n",
        "        model = XGBRanker(objective='rank:pairwise', random_state=run*42)\n",
        "\n",
        "        # Perform hyperparameter tuning using GridSearchCV\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grid,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            cv=3,\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Fit the model with hyperparameter tuning, passing the group column\n",
        "        grid_search.fit(X_train, y_train, group=group_train)\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "\n",
        "        # Make predictions with the best model\n",
        "        predictions = best_model.predict(X_test)\n",
        "\n",
        "        # Ensure predictions match test set length\n",
        "        predictions = predictions[:len(y_test)]\n",
        "\n",
        "        # Calculate NDCG\n",
        "        for k in [5, 10, 20]:\n",
        "            ndcg = ndcg_score(\n",
        "                y_true=y_test.values.reshape(1, -1),\n",
        "                y_score=predictions.reshape(1, -1),\n",
        "                k=k\n",
        "            )\n",
        "            all_metrics[f'ndcg_{k}'].append(ndcg)\n",
        "\n",
        "        # Detailed Ranking Analysis\n",
        "        results = pd.DataFrame({\n",
        "            'True_Score': y_test.reset_index(drop=True),\n",
        "            'Predicted_Score': pd.Series(predictions),\n",
        "            'True_Rank': y_test.reset_index(drop=True).rank(method='dense', ascending=False),\n",
        "            'Predicted_Rank': pd.Series(predictions).rank(method='dense', ascending=False)\n",
        "        })\n",
        "\n",
        "        # Compute Spearman correlation\n",
        "        spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n",
        "\n",
        "        perfect = (results['True_Rank'] == results['Predicted_Rank']).sum()\n",
        "        close = (abs(results['True_Rank'] - results['Predicted_Rank']) <= 1).sum()\n",
        "\n",
        "        all_metrics['spearman'].append(spearman_corr)\n",
        "        all_metrics['perfect'].append(perfect)\n",
        "        all_metrics['close'].append(close)\n",
        "\n",
        "    print(\"\\nAverage Performance:\")\n",
        "    print(\"=\" * 50)\n",
        "    for metric, values in all_metrics.items():\n",
        "        mean = np.mean(values)\n",
        "        std = np.std(values)\n",
        "        if 'ndcg' in metric:\n",
        "            print(f\"{metric.upper()}: {mean:.4f} ± {std:.4f}\")\n",
        "        elif metric == 'spearman':\n",
        "            print(f\"Spearman Correlation: {mean:.3f} ± {std:.3f}\")\n",
        "        else:\n",
        "            print(f\"{metric.title()}: {mean:.1f} ± {std:.1f} out of 10\")\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "# Assuming you have X and y already defined:\n",
        "# X, y = your_feature_data, your_target_data (replace with actual data)\n",
        "\n",
        "# Call the evaluation function\n",
        "metrics_oversample_tuned = run_evaluation_with_tuning(balance_strategy='oversample')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ktKGex_7Voc",
        "outputId": "074468f2-1434-4986-cf37-76d0a27b9059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Run 2/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Run 3/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Run 4/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Run 5/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Run 6/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Run 7/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Run 8/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Run 9/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Run 10/10\n",
            "Balanced Score Distribution:\n",
            "reason_score\n",
            "10.0    5097\n",
            "4.0      566\n",
            "6.5      522\n",
            "7.3      344\n",
            "4.8      307\n",
            "        ... \n",
            "3.8        5\n",
            "1.7        3\n",
            "6.9        3\n",
            "6.2        1\n",
            "5.2        1\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
            "\n",
            "Average Performance:\n",
            "==================================================\n",
            "NDCG_5: 0.8177 ± 0.0050\n",
            "NDCG_10: 0.8177 ± 0.0050\n",
            "NDCG_20: 0.8177 ± 0.0050\n",
            "Spearman Correlation: nan ± nan\n",
            "Perfect: 1021.3 ± 20.0 out of 10\n",
            "Close: 1032.1 ± 19.8 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-73795be178fc>:111: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPTw3w34FLMq",
        "outputId": "4785551b-c75e-43c9-f64a-422e647ec145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import ndcg_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load the data from Google Drive\n",
        "df = pd.read_csv('/content/drive/MyDrive/wos_data/cleaned_dataset.csv')\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Clean numeric features\n",
        "    numeric_features = ['Impact_Factor', 'original_altmetric_score', 'original_cited_by_posts_count']\n",
        "    for col in numeric_features:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Create ratios and interactions with epsilon to avoid division by zero\n",
        "    df['if_alt_ratio'] = df['Impact_Factor'] / (df['original_altmetric_score'] + 1e-5)\n",
        "    df['if_cit_ratio'] = df['Impact_Factor'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "    df['alt_cit_ratio'] = df['original_altmetric_score'] / (df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Log transformations with epsilon\n",
        "    df['log_impact_factor'] = np.log1p(df['Impact_Factor'] + 1e-5)\n",
        "    df['log_altmetric'] = np.log1p(df['original_altmetric_score'] + 1e-5)\n",
        "    df['log_citations'] = np.log1p(df['original_cited_by_posts_count'] + 1e-5)\n",
        "\n",
        "    # Create percentile ranks\n",
        "    df['if_rank'] = df['Impact_Factor'].rank(pct=True)\n",
        "    df['alt_rank'] = df['original_altmetric_score'].rank(pct=True)\n",
        "    df['cit_rank'] = df['original_cited_by_posts_count'].rank(pct=True)\n",
        "\n",
        "    # Create composite scores\n",
        "    df['impact_score'] = (df['if_rank'] + df['alt_rank'] + df['cit_rank']) / 3\n",
        "    df['weighted_impact'] = (df['if_rank'] * 0.4 + df['alt_rank'] * 0.3 + df['cit_rank'] * 0.3)\n",
        "\n",
        "    return df\n",
        "\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "features = [\n",
        "    'Impact_Factor',\n",
        "    'original_altmetric_score',\n",
        "    'original_cited_by_posts_count',\n",
        "    'if_alt_ratio',\n",
        "    'if_cit_ratio',\n",
        "    'alt_cit_ratio',\n",
        "    'log_impact_factor',\n",
        "    'log_altmetric',\n",
        "    'log_citations',\n",
        "    'if_rank',\n",
        "    'alt_rank',\n",
        "    'cit_rank',\n",
        "    'impact_score',\n",
        "    'weighted_impact'\n",
        "]\n",
        "\n",
        "X = df_engineered[features]\n",
        "y = df_engineered['reason_score']\n",
        "X = X.values  # Convert X to numpy array\n",
        "y = y.values\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "def oversample_data(X, y):\n",
        "    unique_scores, counts = np.unique(y, return_counts=True)\n",
        "    max_class_count = np.max(counts)\n",
        "\n",
        "    oversampled_X = []\n",
        "    oversampled_y = []\n",
        "\n",
        "    for unique_score in unique_scores:\n",
        "        samples = X[y == unique_score]\n",
        "        labels = y[y == unique_score]\n",
        "\n",
        "        oversampled_indices = np.random.choice(len(samples), size=max_class_count, replace=True)\n",
        "        oversampled_samples = samples[oversampled_indices]\n",
        "        oversampled_labels = labels[oversampled_indices]\n",
        "\n",
        "        oversampled_X.append(oversampled_samples)\n",
        "        oversampled_y.append(oversampled_labels)\n",
        "\n",
        "    X_oversampled = np.concatenate(oversampled_X)\n",
        "    y_oversampled = np.concatenate(oversampled_y)\n",
        "\n",
        "    return X_oversampled, y_oversampled\n",
        "\n",
        "class RankNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(RankNet, self).__init__()\n",
        "        self.hidden1 = nn.Linear(input_dim, 64)\n",
        "        self.hidden2 = nn.Linear(64, 32)\n",
        "        self.output = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "def train_ranknet(X_train, y_train, X_test, y_test, num_epochs=100, batch_size=32, learning_rate=0.001):\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = RankNet(input_dim)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            X_batch = X_train_tensor[i:i+batch_size]\n",
        "            y_batch = y_train_tensor[i:i+batch_size]\n",
        "\n",
        "            scores = model(X_batch)\n",
        "            loss = criterion(scores.squeeze(), y_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test_tensor).squeeze().numpy()\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "def run_evaluation(n_runs=10):\n",
        "    all_metrics = {\n",
        "        'ndcg_5': [], 'ndcg_10': [], 'ndcg_20': [],\n",
        "        'spearman': [], 'perfect': [], 'close': []\n",
        "    }\n",
        "\n",
        "    print(\"Original Score Distribution:\")\n",
        "    print(np.unique(y, return_counts=True))\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "\n",
        "        X_oversampled, y_oversampled = oversample_data(X_normalized, y)\n",
        "\n",
        "        print(\"Oversampled Score Distribution:\")\n",
        "        print(np.unique(y_oversampled, return_counts=True))\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_oversampled, y_oversampled, test_size=0.2, random_state=run*42\n",
        "        )\n",
        "\n",
        "        predictions = train_ranknet(X_train, y_train, X_test, y_test, learning_rate=0.001)\n",
        "\n",
        "        predictions = predictions[:len(y_test)]\n",
        "\n",
        "        for k in [5, 10, 20]:\n",
        "            ndcg = ndcg_score(\n",
        "                y_true=y_test.reshape(1, -1),\n",
        "                y_score=predictions.reshape(1, -1),\n",
        "                k=k\n",
        "            )\n",
        "            all_metrics[f'ndcg_{k}'].append(ndcg)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            'True_Score': y_test,\n",
        "            'Predicted_Score': pd.Series(predictions),\n",
        "            'True_Rank': pd.Series(y_test).rank(method='dense', ascending=False),\n",
        "            'Predicted_Rank': pd.Series(predictions).rank(method='dense', ascending=False)\n",
        "        })\n",
        "\n",
        "        spearman_corr, _ = spearmanr(results['True_Rank'], results['Predicted_Rank'])\n",
        "\n",
        "        perfect = (results['True_Rank'] == results['Predicted_Rank']).sum()\n",
        "        close = (abs(results['True_Rank'] - results['Predicted_Rank']) <= 1).sum()\n",
        "\n",
        "        all_metrics['spearman'].append(spearman_corr)\n",
        "        all_metrics['perfect'].append(perfect)\n",
        "        all_metrics['close'].append(close)\n",
        "\n",
        "    print(\"\\nAverage Performance:\")\n",
        "    print(\"=\" * 50)\n",
        "    for metric, values in all_metrics.items():\n",
        "        mean = np.mean(values)\n",
        "        std = np.std(values)\n",
        "        if 'ndcg' in metric:\n",
        "            print(f\"{metric.upper()}: {mean:.4f} ± {std:.4f}\")\n",
        "        elif metric == 'spearman':\n",
        "            print(f\"Spearman Correlation: {mean:.3f} ± {std:.3f}\")\n",
        "        else:\n",
        "            print(f\"{metric.title()}: {mean:.1f} ± {std:.1f} out of 10\")\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "# Run evaluation with oversampling\n",
        "print(\"\\n--- Oversampling Strategy ---\")\n",
        "metrics_oversample = run_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cn2sdOLaHudX",
        "outputId": "00137e15-b741-4c84-de5a-581e3b9bc0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Oversampling Strategy ---\n",
            "Original Score Distribution:\n",
            "(array([ 1.5,  1.7,  1.8,  2.5,  2.7,  2.8,  3. ,  3.2,  3.3,  3.5,  3.7,\n",
            "        3.8,  4. ,  4.2,  4.3,  4.5,  4.7,  4.8,  5. ,  5.1,  5.2,  5.3,\n",
            "        5.5,  5.6,  5.7,  5.8,  5.9,  6. ,  6.1,  6.2,  6.3,  6.4,  6.5,\n",
            "        6.6,  6.7,  6.8,  6.9,  7. ,  7.1,  7.2,  7.3,  7.4,  7.5,  7.6,\n",
            "        7.7,  7.8,  7.9,  8. ,  8.1,  8.2,  8.3,  8.4,  8.5,  8.6,  8.7,\n",
            "        8.8,  8.9,  9. ,  9.1,  9.2,  9.3,  9.4,  9.5,  9.6,  9.7,  9.8,\n",
            "        9.9, 10. ]), array([  78,    3,    1,  259,  118,   13,  227,   22,    9,  120,    2,\n",
            "          7,  761,   29,   91,  146,   11,  408,  124,  104,    1,   52,\n",
            "         97,   99,   10,   36,   29,   18,   12,    3,  261,   32,  713,\n",
            "         13,   40,   33,    5,   55,   70,   31,  460,   44,   99,   91,\n",
            "         46,  109,   24,   63,  106,   28,  167,   74,  109,  113,   21,\n",
            "        194,   66,  151,   99,   60,  158,   81,  197,  135,   67,  116,\n",
            "         76, 5097]))\n",
            "\n",
            "Run 1/10\n",
            "Oversampled Score Distribution:\n",
            "(array([ 1.5,  1.7,  1.8,  2.5,  2.7,  2.8,  3. ,  3.2,  3.3,  3.5,  3.7,\n",
            "        3.8,  4. ,  4.2,  4.3,  4.5,  4.7,  4.8,  5. ,  5.1,  5.2,  5.3,\n",
            "        5.5,  5.6,  5.7,  5.8,  5.9,  6. ,  6.1,  6.2,  6.3,  6.4,  6.5,\n",
            "        6.6,  6.7,  6.8,  6.9,  7. ,  7.1,  7.2,  7.3,  7.4,  7.5,  7.6,\n",
            "        7.7,  7.8,  7.9,  8. ,  8.1,  8.2,  8.3,  8.4,  8.5,  8.6,  8.7,\n",
            "        8.8,  8.9,  9. ,  9.1,  9.2,  9.3,  9.4,  9.5,  9.6,  9.7,  9.8,\n",
            "        9.9, 10. ]), array([5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097]))\n",
            "Epoch [10/100], Loss: -13431465984.0000\n",
            "Epoch [20/100], Loss: -108030984192.0000\n",
            "Epoch [30/100], Loss: -365582057472.0000\n",
            "Epoch [40/100], Loss: -868255531008.0000\n",
            "Epoch [50/100], Loss: -1697808121856.0000\n",
            "Epoch [60/100], Loss: -2934292611072.0000\n",
            "Epoch [70/100], Loss: -4641567277056.0000\n",
            "Epoch [80/100], Loss: -6908435496960.0000\n",
            "Epoch [90/100], Loss: -9814216278016.0000\n",
            "Epoch [100/100], Loss: -13438226006016.0000\n",
            "\n",
            "Run 2/10\n",
            "Oversampled Score Distribution:\n",
            "(array([ 1.5,  1.7,  1.8,  2.5,  2.7,  2.8,  3. ,  3.2,  3.3,  3.5,  3.7,\n",
            "        3.8,  4. ,  4.2,  4.3,  4.5,  4.7,  4.8,  5. ,  5.1,  5.2,  5.3,\n",
            "        5.5,  5.6,  5.7,  5.8,  5.9,  6. ,  6.1,  6.2,  6.3,  6.4,  6.5,\n",
            "        6.6,  6.7,  6.8,  6.9,  7. ,  7.1,  7.2,  7.3,  7.4,  7.5,  7.6,\n",
            "        7.7,  7.8,  7.9,  8. ,  8.1,  8.2,  8.3,  8.4,  8.5,  8.6,  8.7,\n",
            "        8.8,  8.9,  9. ,  9.1,  9.2,  9.3,  9.4,  9.5,  9.6,  9.7,  9.8,\n",
            "        9.9, 10. ]), array([5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097]))\n",
            "Epoch [10/100], Loss: -17145013248.0000\n",
            "Epoch [20/100], Loss: -137630875648.0000\n",
            "Epoch [30/100], Loss: -465460854784.0000\n",
            "Epoch [40/100], Loss: -1105051254784.0000\n",
            "Epoch [50/100], Loss: -2160342204416.0000\n",
            "Epoch [60/100], Loss: -3733154955264.0000\n",
            "Epoch [70/100], Loss: -5905338335232.0000\n",
            "Epoch [80/100], Loss: -8789481750528.0000\n",
            "Epoch [90/100], Loss: -12486503825408.0000\n",
            "Epoch [100/100], Loss: -17097330524160.0000\n",
            "\n",
            "Run 3/10\n",
            "Oversampled Score Distribution:\n",
            "(array([ 1.5,  1.7,  1.8,  2.5,  2.7,  2.8,  3. ,  3.2,  3.3,  3.5,  3.7,\n",
            "        3.8,  4. ,  4.2,  4.3,  4.5,  4.7,  4.8,  5. ,  5.1,  5.2,  5.3,\n",
            "        5.5,  5.6,  5.7,  5.8,  5.9,  6. ,  6.1,  6.2,  6.3,  6.4,  6.5,\n",
            "        6.6,  6.7,  6.8,  6.9,  7. ,  7.1,  7.2,  7.3,  7.4,  7.5,  7.6,\n",
            "        7.7,  7.8,  7.9,  8. ,  8.1,  8.2,  8.3,  8.4,  8.5,  8.6,  8.7,\n",
            "        8.8,  8.9,  9. ,  9.1,  9.2,  9.3,  9.4,  9.5,  9.6,  9.7,  9.8,\n",
            "        9.9, 10. ]), array([5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097, 5097,\n",
            "       5097, 5097]))\n",
            "Epoch [10/100], Loss: -13155858432.0000\n",
            "Epoch [20/100], Loss: -105615605760.0000\n",
            "Epoch [30/100], Loss: -357144035328.0000\n",
            "Epoch [40/100], Loss: -848013099008.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4b4c20c33233>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;31m# Run evaluation with oversampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Oversampling Strategy ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0mmetrics_oversample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-4b4c20c33233>\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(n_runs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ranknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-4b4c20c33233>\u001b[0m in \u001b[0;36mtrain_ranknet\u001b[0;34m(X_train, y_train, X_test, y_test, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install pandas scikit-learn imbalanced-learn matplotlib seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QabNkUSCyk6W",
        "outputId": "845e3c2f-b726-4928-f0b2-a3fc7d35db1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vDVIFIsRc9HU",
        "outputId": "9b6365e6-5b01-47c0-9612-7f0a36d9df72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution:\n",
            "severity_category\n",
            "Critical          0.511523\n",
            "Major             0.161568\n",
            "Minor             0.150250\n",
            "Moderate          0.137948\n",
            "Administrative    0.038711\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-validation Scores:\n",
            "[0.21841898 0.2        0.2        0.22061187 0.2       ]\n",
            "Mean CV Score: 0.2078 (+/- 0.0192)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Administrative       0.05      0.07      0.06        94\n",
            "      Critical       0.58      0.50      0.54      1248\n",
            "         Major       0.22      0.26      0.24       394\n",
            "         Minor       0.27      0.26      0.27       367\n",
            "      Moderate       0.24      0.27      0.26       336\n",
            "\n",
            "      accuracy                           0.38      2439\n",
            "     macro avg       0.27      0.28      0.27      2439\n",
            "  weighted avg       0.41      0.38      0.39      2439\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import lightgbm as lgb\n",
        "from datetime import datetime\n",
        "import re\n",
        "from sklearn.metrics import ndcg_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def clean_impact_factor(value):\n",
        "    if pd.isna(value):\n",
        "        return 0.0\n",
        "    if isinstance(value, str):\n",
        "        # Handle '<X' cases\n",
        "        if '<' in value:\n",
        "            return float(value.replace('<', ''))\n",
        "        # Handle other potential string cases\n",
        "        try:\n",
        "            return float(value)\n",
        "        except:\n",
        "            return 0.0\n",
        "    return value\n",
        "\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "def extract_features(df):\n",
        "    # Create date features\n",
        "    df['OriginalPaperDate'] = pd.to_datetime(df['OriginalPaperDate'])\n",
        "    df['RetractionDate'] = pd.to_datetime(df['RetractionDate'])\n",
        "    df['months_to_retraction'] = (df['RetractionDate'] - df['OriginalPaperDate']).dt.total_seconds() / (60*60*24*30.44)\n",
        "\n",
        "    # Publication year and month\n",
        "    df['publication_year'] = df['OriginalPaperDate'].dt.year\n",
        "    df['publication_month'] = df['OriginalPaperDate'].dt.month\n",
        "\n",
        "    # Count authors and institutions\n",
        "    df['author_count'] = df['Author'].str.count(';') + 1\n",
        "    df['institution_count'] = df['Institution'].str.count(';') + 1\n",
        "\n",
        "    # Country features\n",
        "    df['country_count'] = df['Country'].str.count(';') + 1\n",
        "\n",
        "    # Clean Impact Factor\n",
        "    df['Impact_Factor'] = df['Impact_Factor'].apply(clean_impact_factor)\n",
        "\n",
        "    # Journal features (assuming these are from the original data)\n",
        "    df['h_index'] = pd.to_numeric(df['h-index'], errors='coerce')\n",
        "\n",
        "    # Citation and altmetric features\n",
        "    citation_cols = ['original_cited_by_posts_count', 'retraction_cited_by_posts_count']\n",
        "    altmetric_cols = ['original_altmetric_score', 'retraction_altmetric_score']\n",
        "    reader_cols = ['original_reader_count', 'retraction_reader_count']\n",
        "\n",
        "    for col in citation_cols + altmetric_cols + reader_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Calculate citation and altmetric ratios\n",
        "    df['citation_ratio'] = df['retraction_cited_by_posts_count'] / (df['original_cited_by_posts_count'] + 1)\n",
        "    df['altmetric_ratio'] = df['retraction_altmetric_score'] / (df['original_altmetric_score'] + 1)\n",
        "    df['reader_ratio'] = df['retraction_reader_count'] / (df['original_reader_count'] + 1)\n",
        "\n",
        "    # Create severity score mapping\n",
        "    severity_map = {\n",
        "        'Minor': 1,\n",
        "        'Administrative': 2,\n",
        "        'Moderate': 3,\n",
        "        'Major': 4,\n",
        "        'Critical': 5\n",
        "    }\n",
        "    df['severity_score'] = df['severity_category'].map(severity_map)\n",
        "\n",
        "    # Select features for ranking\n",
        "    feature_cols = [\n",
        "        'original_cited_by_posts_count',\n",
        "        'retraction_cited_by_posts_count',\n",
        "        'original_altmetric_score',\n",
        "        'retraction_altmetric_score',\n",
        "        'original_reader_count',\n",
        "        'retraction_reader_count',\n",
        "        'Impact_Factor',\n",
        "        'h_index',\n",
        "        'months_to_retraction',\n",
        "        'author_count',\n",
        "        'institution_count',\n",
        "        'country_count',\n",
        "        'citation_ratio',\n",
        "        'altmetric_ratio',\n",
        "        'reader_ratio'\n",
        "    ]\n",
        "\n",
        "    # Handle missing values\n",
        "    for col in feature_cols:\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "    return df, feature_cols\n",
        "\n",
        "def prepare_ranking_data(df, feature_cols):\n",
        "    # Create query groups (we'll use years as groups)\n",
        "    df['year'] = df['OriginalPaperDate'].dt.year\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = MinMaxScaler()\n",
        "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "    # Prepare data in ranking format\n",
        "    X = df[feature_cols].values\n",
        "    y = df['severity_score'].values\n",
        "    groups = df['year'].values\n",
        "\n",
        "    return X, y, groups\n",
        "\n",
        "def train_ranking_model(X, y, groups, feature_cols):\n",
        "    # Split data into train and validation\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    train_idx, val_idx = next(gkf.split(X, y, groups))\n",
        "\n",
        "    # Prepare LightGBM datasets\n",
        "    train_data = lgb.Dataset(\n",
        "        X[train_idx],\n",
        "        label=y[train_idx],\n",
        "        group=np.bincount(groups[train_idx])[1:],\n",
        "        feature_name=feature_cols\n",
        "    )\n",
        "\n",
        "    val_data = lgb.Dataset(\n",
        "        X[val_idx],\n",
        "        label=y[val_idx],\n",
        "        group=np.bincount(groups[val_idx])[1:],\n",
        "        reference=train_data\n",
        "    )\n",
        "\n",
        "    # Parameters for LightGBM\n",
        "    params = {\n",
        "        'objective': 'lambdarank',\n",
        "        'metric': 'ndcg',\n",
        "        'ndcg_eval_at': [5, 10],\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'min_data_in_leaf': 50,\n",
        "        'min_sum_hessian_in_leaf': 5.0,\n",
        "        'num_threads': 4\n",
        "    }\n",
        "\n",
        "    # Train model\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[train_data, val_data],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
        "    )\n",
        "\n",
        "    return model, train_idx, val_idx\n",
        "\n",
        "def evaluate_model(model, X, y, groups, val_idx):\n",
        "    # Get predictions for validation set\n",
        "    val_preds = model.predict(X[val_idx])\n",
        "\n",
        "    # Calculate NDCG@k for different k values\n",
        "    k_values = [5, 10]\n",
        "    ndcg_scores = {}\n",
        "\n",
        "    for k in k_values:\n",
        "        ndcg = ndcg_score(\n",
        "            y[val_idx].reshape(1, -1),\n",
        "            val_preds.reshape(1, -1),\n",
        "            k=k\n",
        "        )\n",
        "        ndcg_scores[f'ndcg@{k}'] = ndcg\n",
        "\n",
        "    return ndcg_scores, val_preds\n",
        "\n",
        "def analyze_feature_importance(model, feature_cols):\n",
        "    # Get feature importance\n",
        "    importance = model.feature_importance(importance_type='gain')\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': importance\n",
        "    })\n",
        "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(data=feature_importance, x='importance', y='feature')\n",
        "    plt.title('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return feature_importance\n",
        "\n",
        "def rank_new_articles(model, df, feature_cols, n_articles=10):\n",
        "    # Get random sample of articles\n",
        "    sample_df = df.sample(n=n_articles)\n",
        "\n",
        "    # Get predictions\n",
        "    sample_features = sample_df[feature_cols].values\n",
        "    predictions = model.predict(sample_features)\n",
        "\n",
        "    # Add predictions to sample\n",
        "    sample_df['predicted_score'] = predictions\n",
        "\n",
        "    # Sort by predicted score\n",
        "    ranked_articles = sample_df.sort_values('predicted_score', ascending=False)\n",
        "\n",
        "    return ranked_articles[['Title', 'severity_category', 'reason_score', 'predicted_score'] + feature_cols]\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    file_path = '/content/drive/MyDrive/wos_data/cleaned_dataset.csv'\n",
        "    df = load_data(file_path)\n",
        "    df, feature_cols = extract_features(df)\n",
        "\n",
        "    # Prepare ranking data\n",
        "    X, y, groups = prepare_ranking_data(df, feature_cols)\n",
        "\n",
        "    # Train model\n",
        "    model, train_idx, val_idx = train_ranking_model(X, y, groups, feature_cols)\n",
        "\n",
        "    # Evaluate model\n",
        "    ndcg_scores, val_preds = evaluate_model(model, X, y, groups, val_idx)\n",
        "    print(\"\\nModel Performance:\")\n",
        "    for metric, score in ndcg_scores.items():\n",
        "        print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "    # Analyze feature importance\n",
        "    feature_importance = analyze_feature_importance(model, feature_cols)\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    print(feature_importance)\n",
        "\n",
        "    # Rank sample articles\n",
        "    print(\"\\nRanked Sample Articles:\")\n",
        "    ranked_articles = rank_new_articles(model, df, feature_cols)\n",
        "    print(ranked_articles[['Title', 'severity_category', 'reason_score', 'predicted_score']].to_string())\n",
        "\n",
        "    return model, df, feature_cols\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, df, feature_cols = main()"
      ],
      "metadata": {
        "id": "icbvNE19yuph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c53b5ea-e8b9-4e1e-87b3-b0808ae394de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1949\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 12\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[6]\ttraining's ndcg@5: 0.99839\ttraining's ndcg@10: 0.99823\tvalid_1's ndcg@5: 0.99946\tvalid_1's ndcg@10: 0.999461\n",
            "\n",
            "Model Performance:\n",
            "ndcg@5: 0.8000\n",
            "ndcg@10: 0.8545\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importance:\n",
            "                            feature  importance\n",
            "8              months_to_retraction   98.177419\n",
            "0     original_cited_by_posts_count   37.869380\n",
            "6                     Impact_Factor   26.495584\n",
            "2          original_altmetric_score   22.243830\n",
            "9                      author_count   18.884805\n",
            "13                  altmetric_ratio    6.789205\n",
            "10                institution_count    5.834660\n",
            "12                   citation_ratio    1.529893\n",
            "1   retraction_cited_by_posts_count    1.275460\n",
            "3        retraction_altmetric_score    0.000000\n",
            "4             original_reader_count    0.000000\n",
            "5           retraction_reader_count    0.000000\n",
            "7                           h_index    0.000000\n",
            "11                    country_count    0.000000\n",
            "14                     reader_ratio    0.000000\n",
            "\n",
            "Ranked Sample Articles:\n",
            "                                                                                                                                                                                                      Title severity_category  reason_score  predicted_score\n",
            "4305                                                                   Aberrant frequency of IL-10-producing b cells and its association with Treg and MDSC cells in Non Small Cell Lung Carcinoma patients          Critical          10.0         0.461404\n",
            "5676                                                                      AMP-activated protein kinase is involved in induction of protective autophagy in astrocytes exposed to oxygen-glucose deprivation    Administrative           2.5         0.287842\n",
            "10936                                                                                                   Population genetics of 24 Y-STR loci in Chinese Han population from Jilin Province, Northeast China          Critical          10.0         0.019420\n",
            "11656                                                                                                                                                          Impact of ANN in Revealing of Viral Peptides          Critical          10.0        -0.085225\n",
            "2298                                                                                                                                           Cannibalism by damselflies increases with rising temperature             Minor           3.0        -0.108975\n",
            "11721                                                                                                      A Deep Learning Framework for Earlier Prediction of Diabetic Retinopathy from Fundus Photographs          Critical          10.0        -0.147892\n",
            "2353                                                                                                                                          Opioid switching to improve pain relief and drug tolerability             Minor           4.8        -0.239098\n",
            "785                                                                           RORα Suppresses Epithelial-to-Mesenchymal Transition and Invasion in Human Gastric Cancer Cells via the Wnt/β-Catenin Pathway             Major           7.6        -0.239766\n",
            "11640  NORAD Promotes the Viability, Migration, and Phenotypic Switch of Human Vascular Smooth Muscle Cells during Aortic Dissection via LIN28B-Mediated TGF-β Promotion and Subsequent Enhanced Glycolysis          Critical          10.0        -0.318911\n",
            "9614                 Changes in depression and suicidal ideation under severe lockdown restrictions during the first wave of the COVID-19 pandemic in Spain: a longitudinal study in the general population             Minor           4.0        -0.465338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import lightgbm as lgb\n",
        "from datetime import datetime\n",
        "import re\n",
        "from sklearn.metrics import ndcg_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def clean_impact_factor(value):\n",
        "    if pd.isna(value):\n",
        "        return 0.0\n",
        "    if isinstance(value, str):\n",
        "        if '<' in value:\n",
        "            return float(value.replace('<', ''))\n",
        "        try:\n",
        "            return float(value)\n",
        "        except:\n",
        "            return 0.0\n",
        "    return value\n",
        "\n",
        "def extract_features(df):\n",
        "    # Create date features\n",
        "    df['OriginalPaperDate'] = pd.to_datetime(df['OriginalPaperDate'])\n",
        "    df['RetractionDate'] = pd.to_datetime(df['RetractionDate'])\n",
        "    df['months_to_retraction'] = (df['RetractionDate'] - df['OriginalPaperDate']).dt.total_seconds() / (60*60*24*30.44)\n",
        "\n",
        "    # Publication features\n",
        "    df['publication_year'] = df['OriginalPaperDate'].dt.year\n",
        "    df['publication_month'] = df['OriginalPaperDate'].dt.month\n",
        "    current_year = 2025  # As per the context date\n",
        "    df['years_since_publication'] = current_year - df['publication_year']\n",
        "\n",
        "    # Author and institution features\n",
        "    df['author_count'] = df['Author'].str.count(';') + 1\n",
        "    df['institution_count'] = df['Institution'].str.count(';') + 1\n",
        "    df['country_count'] = df['Country'].str.count(';') + 1\n",
        "\n",
        "    # Clean Impact Factor\n",
        "    df['Impact_Factor'] = df['Impact_Factor'].apply(clean_impact_factor)\n",
        "    df['h_index'] = pd.to_numeric(df['h-index'], errors='coerce').fillna(0)\n",
        "\n",
        "    # Citation and attention metrics with time normalization\n",
        "    metrics_cols = ['original_cited_by_posts_count', 'retraction_cited_by_posts_count',\n",
        "                   'original_altmetric_score', 'retraction_altmetric_score',\n",
        "                   'original_reader_count', 'retraction_reader_count']\n",
        "\n",
        "    for col in metrics_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Normalize metrics by time\n",
        "    df['citations_per_year'] = df['original_cited_by_posts_count'] / (df['years_since_publication'] + 1)\n",
        "    df['altmetric_per_year'] = df['original_altmetric_score'] / (df['years_since_publication'] + 1)\n",
        "    df['readers_per_year'] = df['original_reader_count'] / (df['years_since_publication'] + 1)\n",
        "\n",
        "    # Create inverse metrics (assuming higher metrics might indicate less severity)\n",
        "    df['inverse_citations'] = 1 / (df['citations_per_year'] + 1)\n",
        "    df['inverse_altmetric'] = 1 / (df['altmetric_per_year'] + 1)\n",
        "    df['inverse_impact'] = 1 / (df['Impact_Factor'] + 1)\n",
        "    df['inverse_h_index'] = 1 / (df['h_index'] + 1)\n",
        "\n",
        "    # Attention change ratios\n",
        "    df['citation_increase'] = (df['retraction_cited_by_posts_count'] - df['original_cited_by_posts_count']) / (df['original_cited_by_posts_count'] + 1)\n",
        "    df['altmetric_increase'] = (df['retraction_altmetric_score'] - df['original_altmetric_score']) / (df['original_altmetric_score'] + 1)\n",
        "    df['reader_increase'] = (df['retraction_reader_count'] - df['original_reader_count']) / (df['original_reader_count'] + 1)\n",
        "\n",
        "    # Map severity categories\n",
        "    severity_map = {\n",
        "        'Minor': 1,\n",
        "        'Administrative': 2,\n",
        "        'Moderate': 3,\n",
        "        'Major': 4,\n",
        "        'Critical': 5\n",
        "    }\n",
        "    df['severity_score'] = df['severity_category'].map(severity_map)\n",
        "\n",
        "    # Select features for ranking\n",
        "    feature_cols = [\n",
        "        'inverse_citations',\n",
        "        'inverse_altmetric',\n",
        "        'inverse_impact',\n",
        "        'inverse_h_index',\n",
        "        'citation_increase',\n",
        "        'altmetric_increase',\n",
        "        'reader_increase',\n",
        "        'months_to_retraction',\n",
        "        'years_since_publication',\n",
        "        'author_count',\n",
        "        'institution_count',\n",
        "        'country_count'\n",
        "    ]\n",
        "\n",
        "    # Handle missing values\n",
        "    for col in feature_cols:\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "    return df, feature_cols\n",
        "\n",
        "def prepare_ranking_data(df, feature_cols):\n",
        "    # Create query groups by year\n",
        "    df['year'] = df['OriginalPaperDate'].dt.year\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = MinMaxScaler()\n",
        "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "    X = df[feature_cols].values\n",
        "    y = df['severity_score'].values\n",
        "    groups = df['year'].values\n",
        "\n",
        "    return X, y, groups\n",
        "\n",
        "def train_ranking_model(X, y, groups, feature_cols):\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    train_idx, val_idx = next(gkf.split(X, y, groups))\n",
        "\n",
        "    train_data = lgb.Dataset(\n",
        "        X[train_idx],\n",
        "        label=y[train_idx],\n",
        "        group=np.bincount(groups[train_idx])[1:],\n",
        "        feature_name=feature_cols\n",
        "    )\n",
        "\n",
        "    val_data = lgb.Dataset(\n",
        "        X[val_idx],\n",
        "        label=y[val_idx],\n",
        "        group=np.bincount(groups[val_idx])[1:],\n",
        "        reference=train_data\n",
        "    )\n",
        "\n",
        "    params = {\n",
        "        'objective': 'lambdarank',\n",
        "        'metric': 'ndcg',\n",
        "        'ndcg_eval_at': [5, 10],\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'min_data_in_leaf': 50,\n",
        "        'min_sum_hessian_in_leaf': 5.0,\n",
        "        'num_threads': 4\n",
        "    }\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[train_data, val_data],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
        "    )\n",
        "\n",
        "    return model, train_idx, val_idx\n",
        "\n",
        "def evaluate_and_analyze(model, X, y, groups, val_idx, feature_cols):\n",
        "    # Get predictions\n",
        "    val_preds = model.predict(X[val_idx])\n",
        "\n",
        "    # Calculate NDCG scores\n",
        "    ndcg_scores = {}\n",
        "    for k in [5, 10]:\n",
        "        ndcg = ndcg_score(\n",
        "            y[val_idx].reshape(1, -1),\n",
        "            val_preds.reshape(1, -1),\n",
        "            k=k\n",
        "        )\n",
        "        ndcg_scores[f'ndcg@{k}'] = ndcg\n",
        "\n",
        "    # Feature importance analysis\n",
        "    importance = model.feature_importance(importance_type='gain')\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(data=feature_importance, x='importance', y='feature')\n",
        "    plt.title('Feature Importance in Severity Prediction')\n",
        "    plt.xlabel('Importance Score')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return ndcg_scores, val_preds, feature_importance\n",
        "\n",
        "def rank_new_articles(model, df, feature_cols, n_articles=10):\n",
        "    sample_df = df.sample(n=n_articles)\n",
        "    predictions = model.predict(sample_df[feature_cols].values)\n",
        "    sample_df['predicted_score'] = predictions\n",
        "\n",
        "    ranked_articles = sample_df.sort_values('predicted_score', ascending=False)\n",
        "\n",
        "    display_cols = ['Title', 'severity_category', 'reason_score', 'predicted_score',\n",
        "                   'original_cited_by_posts_count', 'original_altmetric_score', 'Impact_Factor']\n",
        "\n",
        "    return ranked_articles[display_cols]\n",
        "\n",
        "def main():\n",
        "    file_path = '/content/drive/MyDrive/wos_data/cleaned_dataset.csv'\n",
        "    df = pd.read_csv(file_path)\n",
        "    df, feature_cols = extract_features(df)\n",
        "\n",
        "    X, y, groups = prepare_ranking_data(df, feature_cols)\n",
        "    model, train_idx, val_idx = train_ranking_model(X, y, groups, feature_cols)\n",
        "\n",
        "    ndcg_scores, val_preds, feature_importance = evaluate_and_analyze(\n",
        "        model, X, y, groups, val_idx, feature_cols\n",
        "    )\n",
        "\n",
        "    print(\"\\nModel Performance:\")\n",
        "    for metric, score in ndcg_scores.items():\n",
        "        print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    print(feature_importance)\n",
        "\n",
        "    print(\"\\nRanked Sample Articles:\")\n",
        "    ranked_articles = rank_new_articles(model, df, feature_cols)\n",
        "    print(ranked_articles.to_string())\n",
        "\n",
        "    return model, df, feature_cols\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, df, feature_cols = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4f_nzcVdCfPs",
        "outputId": "3dd2ff06-cc9d-47f5-c869-583c7521b7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1601\n",
            "[LightGBM] [Info] Number of data points in the train set: 9755, number of used features: 11\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[2]\ttraining's ndcg@5: 0.997953\ttraining's ndcg@10: 0.997701\tvalid_1's ndcg@5: 0.999538\tvalid_1's ndcg@10: 0.999421\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance:\n",
            "ndcg@5: 0.9231\n",
            "ndcg@10: 0.9231\n",
            "\n",
            "Feature Importance:\n",
            "                    feature  importance\n",
            "2            inverse_impact   18.612880\n",
            "0         inverse_citations   14.924237\n",
            "7      months_to_retraction   14.184092\n",
            "5        altmetric_increase    6.729713\n",
            "4         citation_increase    6.288730\n",
            "8   years_since_publication    5.945129\n",
            "9              author_count    5.200723\n",
            "10        institution_count    2.343310\n",
            "3           inverse_h_index    1.614610\n",
            "1         inverse_altmetric    1.378945\n",
            "6           reader_increase    0.000000\n",
            "11            country_count    0.000000\n",
            "\n",
            "Ranked Sample Articles:\n",
            "                                                                                                                                                                         Title severity_category  reason_score  predicted_score  original_cited_by_posts_count  original_altmetric_score  Impact_Factor\n",
            "6869                                                                         Knockdown of microRNA-203 alleviates LPS-induced injury by targeting MCL-1 in C28/I2 chondrocytes          Critical          10.0         0.222755                            1.0                     0.250            3.3\n",
            "11959                                           The effects of thymoquinone on memory impairment and inflammation in rats with hepatic encephalopathy induced by thioacetamide          Critical          10.0         0.133831                            1.0                     1.000            3.2\n",
            "3325                                                                                                 Application of Food-specific IgG Antibody Detection in Allergy Dermatosis          Moderate           6.5         0.076368                            2.0                     3.500            1.7\n",
            "8182                                                                                                                        First Person Account: My Dream Life, A Normal Life    Administrative           1.5         0.007569                            1.0                     0.250            5.3\n",
            "3278                                                                          Mobile Cloud Computing: The Taxonomy and Comparison of Mobile Cloud Computing Application Models             Major           8.1        -0.038657                            2.0                     0.250            1.9\n",
            "9972                                                               Construct comprehensive indicators through a signal extraction approach for predicting housing price crises          Critical           9.5        -0.063241                            1.0                     8.000            2.9\n",
            "979                                                                                                Fattening Fasting: Hungry Grocery Shoppers Buy More Calories, Not More Food          Critical           9.2        -0.114907                          175.0                   415.074           22.5\n",
            "5139                                CuO and CeO2 Nanostructures Green Synthesized Using Olive Leaf Extract Inhibits the Growth of Highly Virulent Multidrug Resistant Bacteria          Critical          10.0        -0.116992                            7.0                     8.830            4.4\n",
            "3249   Clinical outcomes of complete versus lesion only primary percutaneous coronary revascularization for multivessel disease and ST-segment elevation myocardial infarction             Major           8.1        -0.129239                            4.0                     8.640            1.4\n",
            "4298                                                                                  Prognostic Relevance of Short-Term Blood Pressure Variability: The Spanish ABPM Registry    Administrative           2.5        -0.232711                            5.0                    11.450            6.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import lightgbm as lgb\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import ndcg_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "\n",
        "def clean_impact_factor(value):\n",
        "    if pd.isna(value):\n",
        "        return 0.0\n",
        "    if isinstance(value, str):\n",
        "        if '<' in value:\n",
        "            return float(value.replace('<', ''))\n",
        "        try:\n",
        "            return float(value)\n",
        "        except:\n",
        "            return 0.0\n",
        "    return value\n",
        "\n",
        "def extract_features(df):\n",
        "    # Create date features\n",
        "    df['OriginalPaperDate'] = pd.to_datetime(df['OriginalPaperDate'])\n",
        "    df['RetractionDate'] = pd.to_datetime(df['RetractionDate'])\n",
        "    df['months_to_retraction'] = (df['RetractionDate'] - df['OriginalPaperDate']).dt.total_seconds() / (60*60*24*30.44)\n",
        "\n",
        "    # Publication features\n",
        "    df['publication_year'] = df['OriginalPaperDate'].dt.year\n",
        "    current_year = 2025\n",
        "    df['years_since_publication'] = current_year - df['publication_year']\n",
        "\n",
        "    # Clean and transform metrics\n",
        "    df['Impact_Factor'] = df['Impact_Factor'].apply(clean_impact_factor)\n",
        "\n",
        "    # Log transformations with handling of zeros/negatives\n",
        "    eps = 1e-8\n",
        "    df['log_impact'] = np.log1p(df['Impact_Factor'] + eps)\n",
        "    df['log_citations'] = np.log1p(df['original_cited_by_posts_count'] + eps)\n",
        "\n",
        "    # Time-weighted metrics\n",
        "    time_factor = np.sqrt(df['years_since_publication'] + 1)\n",
        "    df['time_adjusted_impact'] = df['Impact_Factor'] / time_factor\n",
        "    df['time_adjusted_citations'] = df['original_cited_by_posts_count'] / time_factor\n",
        "\n",
        "    # Map severity\n",
        "    severity_map = {\n",
        "        'Minor': 1,\n",
        "        'Administrative': 2,\n",
        "        'Moderate': 3,\n",
        "        'Major': 4,\n",
        "        'Critical': 5\n",
        "    }\n",
        "    df['severity_score'] = df['severity_category'].map(severity_map)\n",
        "\n",
        "    # Select features\n",
        "    feature_cols = [\n",
        "        'log_impact',\n",
        "        'time_adjusted_impact',\n",
        "        'years_since_publication'\n",
        "    ]\n",
        "\n",
        "    # Handle missing values\n",
        "    for col in feature_cols:\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "    return df, feature_cols\n",
        "\n",
        "def train_ranking_model(X, y, groups, feature_cols):\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    train_idx, val_idx = next(gkf.split(X, y, groups))\n",
        "\n",
        "    train_data = lgb.Dataset(\n",
        "        X[train_idx],\n",
        "        label=y[train_idx],\n",
        "        group=np.bincount(groups[train_idx])[1:],\n",
        "        feature_name=feature_cols\n",
        "    )\n",
        "\n",
        "    val_data = lgb.Dataset(\n",
        "        X[val_idx],\n",
        "        label=y[val_idx],\n",
        "        group=np.bincount(groups[val_idx])[1:],\n",
        "        reference=train_data\n",
        "    )\n",
        "\n",
        "    params = {\n",
        "        'objective': 'lambdarank',\n",
        "        'metric': ['ndcg', 'map'],\n",
        "        'ndcg_eval_at': [5, 10],\n",
        "        'learning_rate': 0.01,\n",
        "        'num_leaves': 7,\n",
        "        'max_depth': 3,\n",
        "        'min_data_in_leaf': 50,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'lambda_l1': 0.5,\n",
        "        'lambda_l2': 0.5,\n",
        "        'min_gain_to_split': 0.2,\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=500,\n",
        "        valid_sets=[train_data, val_data],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=50),\n",
        "            lgb.log_evaluation(period=50)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model, train_idx, val_idx\n",
        "\n",
        "def evaluate_model(model, X, y, val_idx):\n",
        "    val_preds = model.predict(X[val_idx])\n",
        "\n",
        "    metrics = {}\n",
        "    for k in [5, 10, 20]:\n",
        "        metrics[f'ndcg@{k}'] = ndcg_score(\n",
        "            y[val_idx].reshape(1, -1),\n",
        "            val_preds.reshape(1, -1),\n",
        "            k=k\n",
        "        )\n",
        "\n",
        "    spearman_corr, _ = spearmanr(y[val_idx], val_preds)\n",
        "    kendall_corr, _ = kendalltau(y[val_idx], val_preds)\n",
        "    metrics['spearman'] = spearman_corr\n",
        "    metrics['kendall'] = kendall_corr\n",
        "\n",
        "    return metrics, val_preds\n",
        "\n",
        "def analyze_feature_importance(model, feature_cols):\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': model.feature_importance(importance_type='gain')\n",
        "    })\n",
        "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(data=importance_df, x='importance', y='feature')\n",
        "    plt.title('Feature Importance (Gain)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "def rank_new_articles(model, df, feature_cols, n_articles=10):\n",
        "    sample_df = df.sample(n=n_articles)\n",
        "    predictions = model.predict(sample_df[feature_cols].values)\n",
        "\n",
        "    sample_df['predicted_score'] = predictions\n",
        "\n",
        "    # Use rank with method='first' to handle ties\n",
        "    rank_pct = pd.Series(predictions).rank(pct=True, method='first')\n",
        "\n",
        "    # Define severity based on rank percentiles\n",
        "    sample_df['predicted_severity'] = pd.cut(\n",
        "        rank_pct,\n",
        "        bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
        "        labels=['Minor', 'Administrative', 'Moderate', 'Major', 'Critical'],\n",
        "        include_lowest=True\n",
        "    )\n",
        "\n",
        "    ranked_articles = sample_df.sort_values('predicted_score', ascending=False)\n",
        "\n",
        "    display_cols = [\n",
        "        'Title',\n",
        "        'severity_category',\n",
        "        'predicted_severity',\n",
        "        'reason_score',\n",
        "        'predicted_score',\n",
        "        'Impact_Factor',\n",
        "        'original_cited_by_posts_count'\n",
        "    ]\n",
        "\n",
        "    return ranked_articles[display_cols]\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    file_path = '/content/drive/MyDrive/wos_data/cleaned_dataset.csv'\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract features\n",
        "    print(\"Extracting features...\")\n",
        "    df, feature_cols = extract_features(df)\n",
        "\n",
        "    # Prepare data\n",
        "    X = df[feature_cols].values\n",
        "    y = df['severity_score'].values\n",
        "    groups = df['publication_year'].values\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    model, train_idx, val_idx = train_ranking_model(X, y, groups, feature_cols)\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"Evaluating model...\")\n",
        "    metrics, val_preds = evaluate_model(model, X, y, val_idx)\n",
        "    print(\"\\nModel Performance:\")\n",
        "    for metric, score in metrics.items():\n",
        "        print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "    # Analyze importance\n",
        "    print(\"\\nAnalyzing feature importance...\")\n",
        "    importance_df = analyze_feature_importance(model, feature_cols)\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    print(importance_df)\n",
        "\n",
        "    # Rank examples\n",
        "    print(\"\\nGenerating sample rankings...\")\n",
        "    ranked_articles = rank_new_articles(model, df, feature_cols)\n",
        "    print(\"\\nRanked Sample Articles:\")\n",
        "    print(ranked_articles.to_string())\n",
        "\n",
        "    return model, df, feature_cols\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, df, feature_cols = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GK7HG2D7Dw-p",
        "outputId": "d6ff0cb7-8f8e-459a-a6a0-fad2c896616a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Extracting features...\n",
            "Training model...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttraining's ndcg@5: 0.996768\ttraining's ndcg@10: 0.996867\ttraining's map@5: 1\ttraining's map@10: 1\tvalid_1's ndcg@5: 0.998953\tvalid_1's ndcg@10: 0.99901\tvalid_1's map@5: 1\tvalid_1's map@10: 1\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's ndcg@5: 0.994\ttraining's ndcg@10: 0.994847\ttraining's map@5: 1\ttraining's map@10: 1\tvalid_1's ndcg@5: 0.998983\tvalid_1's ndcg@10: 0.998874\tvalid_1's map@5: 1\tvalid_1's map@10: 1\n",
            "Evaluating model...\n",
            "\n",
            "Model Performance:\n",
            "ndcg@5: 0.7896\n",
            "ndcg@10: 0.7896\n",
            "ndcg@20: 0.7896\n",
            "spearman: 0.1502\n",
            "kendall: 0.1294\n",
            "\n",
            "Analyzing feature importance...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importance:\n",
            "                   feature  importance\n",
            "1     time_adjusted_impact    5.637754\n",
            "0               log_impact    4.656070\n",
            "2  years_since_publication    0.000000\n",
            "\n",
            "Generating sample rankings...\n",
            "\n",
            "Ranked Sample Articles:\n",
            "                                                                                                                           Title severity_category predicted_severity  reason_score  predicted_score  Impact_Factor  original_cited_by_posts_count\n",
            "8316                                          Epigenetic Regulator Polycomb Group Protein Complexes Control Cell Fate and Cancer          Critical                NaN           9.1         0.004759            4.5                            1.0\n",
            "4508                                   Effect of miR-451 on the Biological Behavior of the Esophageal Carcinoma Cell Line EC9706          Critical                NaN          10.0         0.004759            2.5                            1.0\n",
            "1312                               Factors associated with post-stroke depression and fatigue: lesion location and coping styles          Moderate                NaN           6.5         0.004759            4.8                            1.0\n",
            "3018                               Src inhibition blocks renal interstitial fibroblast activation and ameliorates renal fibrosis    Administrative                NaN           2.5        -0.003936           14.8                           11.0\n",
            "4013                                                                                              Quantized Majorana conductance          Critical                NaN           9.7        -0.003936           50.5                          171.0\n",
            "11624  Knockdown of insulin-like growth factor 1 exerts a protective effect on hypoxic injury of aged BM-MSCs: role of autophagy          Critical                NaN          10.0        -0.003936            7.1                            1.0\n",
            "962        A fluorescent molecularly-imprinted polymer gate with temperature and pH as inputs for detection of alpha-fetoprotein          Critical                NaN          10.0        -0.003936           10.7                            5.0\n",
            "491        Review of the Emerging Evidence Demonstrating the Efficacy of Ivermectin in the Prophylaxis and Treatment of COVID-19          Moderate                NaN           5.6        -0.003936            4.4                         5664.0\n",
            "10535                                                 Role of Quercetin in Preventing Thioacetamide-Induced Liver Injury in Rats          Critical                NaN          10.0        -0.004726            1.4                            1.0\n",
            "1616          Primary parasitoid and hyperparasitoid guilds (hymenoptera) of grain aphid (Sitobion avenae F.) in northern Poland          Moderate                NaN           6.3        -0.004726            0.7                            1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import lightgbm as lgb\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import ndcg_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess text data.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_title_features(df):\n",
        "    \"\"\"Extract features from paper titles.\"\"\"\n",
        "    # Preprocess titles\n",
        "    df['cleaned_title'] = df['Title'].apply(preprocess_text)\n",
        "\n",
        "    # TF-IDF features\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=100,\n",
        "        stop_words='english',\n",
        "        ngram_range=(1, 2)\n",
        "    )\n",
        "\n",
        "    # Get TF-IDF matrix\n",
        "    title_features = tfidf.fit_transform(df['cleaned_title'])\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    title_feature_names = [f'title_tfidf_{i}' for i in range(title_features.shape[1])]\n",
        "    title_features_df = pd.DataFrame(\n",
        "        title_features.toarray(),\n",
        "        columns=title_feature_names\n",
        "    )\n",
        "\n",
        "    # Basic text statistics\n",
        "    df['title_word_count'] = df['cleaned_title'].apply(lambda x: len(x.split()))\n",
        "    df['title_unique_word_count'] = df['cleaned_title'].apply(lambda x: len(set(x.split())))\n",
        "    df['title_word_density'] = df['title_unique_word_count'] / (df['title_word_count'] + 1)\n",
        "\n",
        "    return pd.concat([df, title_features_df], axis=1), title_feature_names\n",
        "\n",
        "def clean_impact_factor(value):\n",
        "    \"\"\"Clean impact factor values.\"\"\"\n",
        "    if pd.isna(value):\n",
        "        return 0.0\n",
        "    if isinstance(value, str):\n",
        "        if '<' in value:\n",
        "            return float(value.replace('<', ''))\n",
        "        try:\n",
        "            return float(value)\n",
        "        except:\n",
        "            return 0.0\n",
        "    return value\n",
        "\n",
        "def extract_features(df):\n",
        "    \"\"\"Extract all features from the dataset.\"\"\"\n",
        "    # Create date features\n",
        "    df['OriginalPaperDate'] = pd.to_datetime(df['OriginalPaperDate'])\n",
        "    df['RetractionDate'] = pd.to_datetime(df['RetractionDate'])\n",
        "\n",
        "    # Time-based features\n",
        "    df['months_to_retraction'] = (df['RetractionDate'] - df['OriginalPaperDate']).dt.total_seconds() / (60*60*24*30.44)\n",
        "    df['publication_year'] = df['OriginalPaperDate'].dt.year\n",
        "    df['publication_month'] = df['OriginalPaperDate'].dt.month\n",
        "    current_year = 2025\n",
        "    df['years_since_publication'] = current_year - df['publication_year']\n",
        "\n",
        "    # Clean and transform metrics\n",
        "    df['Impact_Factor'] = df['Impact_Factor'].apply(clean_impact_factor)\n",
        "\n",
        "    # Extract title features\n",
        "    df, title_feature_names = extract_title_features(df)\n",
        "\n",
        "    # Advanced feature engineering\n",
        "    eps = 1e-8\n",
        "    df['log_impact'] = np.log1p(df['Impact_Factor'] + eps)\n",
        "    df['log_citations'] = np.log1p(df['original_cited_by_posts_count'] + eps)\n",
        "\n",
        "    # Interaction features\n",
        "    df['impact_citation_ratio'] = df['Impact_Factor'] / (df['original_cited_by_posts_count'] + 1)\n",
        "    df['citations_per_year'] = df['original_cited_by_posts_count'] / (df['years_since_publication'] + 1)\n",
        "\n",
        "    # Time-weighted metrics\n",
        "    time_factor = np.sqrt(df['years_since_publication'] + 1)\n",
        "    df['time_adjusted_impact'] = df['Impact_Factor'] / time_factor\n",
        "    df['time_adjusted_citations'] = df['original_cited_by_posts_count'] / time_factor\n",
        "\n",
        "    # Severity mapping with balanced weights\n",
        "    severity_counts = df['severity_category'].value_counts()\n",
        "    severity_weights = {\n",
        "        category: 1 / (count / len(df))\n",
        "        for category, count in severity_counts.items()\n",
        "    }\n",
        "\n",
        "    severity_map = {\n",
        "        'Minor': 1,\n",
        "        'Administrative': 2,\n",
        "        'Moderate': 3,\n",
        "        'Major': 4,\n",
        "        'Critical': 5\n",
        "    }\n",
        "    df['severity_score'] = df['severity_category'].map(severity_map)\n",
        "    df['severity_weight'] = df['severity_category'].map(severity_weights)\n",
        "\n",
        "    # Select features\n",
        "    base_features = [\n",
        "        'log_impact',\n",
        "        'log_citations',\n",
        "        'time_adjusted_impact',\n",
        "        'time_adjusted_citations',\n",
        "        'impact_citation_ratio',\n",
        "        'citations_per_year',\n",
        "        'years_since_publication',\n",
        "        'months_to_retraction',\n",
        "        'publication_month',\n",
        "        'title_word_count',\n",
        "        'title_unique_word_count',\n",
        "        'title_word_density'\n",
        "    ]\n",
        "\n",
        "    feature_cols = base_features + title_feature_names\n",
        "\n",
        "    # Handle missing values\n",
        "    for col in feature_cols:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # Scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "    return df, feature_cols\n",
        "\n",
        "def train_ranking_model(X, y, groups, weights, feature_cols):\n",
        "    \"\"\"Train the ranking model with cross-validation.\"\"\"\n",
        "    # Convert inputs to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    groups = np.array(groups)\n",
        "    weights = np.array(weights)\n",
        "\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    scores = []\n",
        "    models = []\n",
        "\n",
        "    # Cross validation\n",
        "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n",
        "        print(f\"\\nTraining fold {fold + 1}/5...\")\n",
        "\n",
        "        train_data = lgb.Dataset(\n",
        "            X[train_idx],\n",
        "            label=y[train_idx],\n",
        "            weight=weights[train_idx],\n",
        "            group=np.bincount(groups[train_idx])[1:],\n",
        "            feature_name=feature_cols\n",
        "        )\n",
        "\n",
        "        val_data = lgb.Dataset(\n",
        "            X[val_idx],\n",
        "            label=y[val_idx],\n",
        "            weight=weights[val_idx],\n",
        "            group=np.bincount(groups[val_idx])[1:],\n",
        "            reference=train_data\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            'objective': 'lambdarank',\n",
        "            'metric': ['ndcg', 'map'],\n",
        "            'ndcg_eval_at': [5, 10, 20],\n",
        "            'learning_rate': 0.003,\n",
        "            'num_leaves': 63,\n",
        "            'max_depth': 8,\n",
        "            'min_data_in_leaf': 20,\n",
        "            'feature_fraction': 0.8,\n",
        "            'bagging_fraction': 0.8,\n",
        "            'bagging_freq': 5,\n",
        "            'lambda_l1': 0.05,\n",
        "            'lambda_l2': 0.05,\n",
        "            'min_gain_to_split': 0.05,\n",
        "            'verbose': -1\n",
        "        }\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            num_boost_round=2000,\n",
        "            valid_sets=[train_data, val_data],\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=150),\n",
        "                lgb.log_evaluation(period=100)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Evaluate fold\n",
        "        val_preds = model.predict(X[val_idx])\n",
        "        fold_metrics = calculate_metrics(y[val_idx], val_preds)\n",
        "        scores.append(fold_metrics)\n",
        "        models.append((model, val_idx))\n",
        "\n",
        "        print(f\"Fold {fold + 1} metrics:\")\n",
        "        for metric, score in fold_metrics.items():\n",
        "            print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "    # Select best model\n",
        "    best_idx = np.argmax([s['ndcg@10'] for s in scores])\n",
        "    best_model, best_val_idx = models[best_idx]\n",
        "\n",
        "    print(f\"\\nSelected best model from fold {best_idx + 1}\")\n",
        "\n",
        "    return best_model, best_val_idx, scores\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate comprehensive ranking metrics.\"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # NDCG at different k\n",
        "    for k in [5, 10, 20, 50]:\n",
        "        metrics[f'ndcg@{k}'] = ndcg_score(\n",
        "            y_true.reshape(1, -1),\n",
        "            y_pred.reshape(1, -1),\n",
        "            k=k\n",
        "        )\n",
        "\n",
        "    # Correlation metrics\n",
        "    spearman_corr, _ = spearmanr(y_true, y_pred)\n",
        "    kendall_corr, _ = kendalltau(y_true, y_pred)\n",
        "    metrics['spearman'] = spearman_corr\n",
        "    metrics['kendall'] = kendall_corr\n",
        "\n",
        "    # Ranking metrics\n",
        "    y_true_ranks = pd.Series(y_true).rank()\n",
        "    y_pred_ranks = pd.Series(y_pred).rank()\n",
        "    metrics['rank_mae'] = np.mean(np.abs(y_true_ranks - y_pred_ranks))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def evaluate_model(model, X, y, val_idx, feature_cols):\n",
        "    \"\"\"Evaluate model performance and analyze feature importance.\"\"\"\n",
        "    # Convert inputs to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    val_idx = np.array(val_idx)\n",
        "\n",
        "    # Ensure val_idx is boolean or integer array\n",
        "    if val_idx.dtype == bool:\n",
        "        val_preds = model.predict(X[val_idx])\n",
        "        val_true = y[val_idx]\n",
        "    else:\n",
        "        val_preds = model.predict(X[val_idx.astype(int)])\n",
        "        val_true = y[val_idx.astype(int)]\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(val_true, val_preds)\n",
        "\n",
        "    # Feature importance analysis\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': model.feature_importance(importance_type='gain')\n",
        "    })\n",
        "\n",
        "    # Add feature categories\n",
        "    def get_feature_category(feature_name):\n",
        "        if feature_name.startswith('title_tfidf_'):\n",
        "            return 'Title TF-IDF'\n",
        "        elif feature_name.startswith('title_'):\n",
        "            return 'Text Statistics'\n",
        "        elif 'citation' in feature_name:\n",
        "            return 'Citation Metrics'\n",
        "        elif 'impact' in feature_name:\n",
        "            return 'Impact Metrics'\n",
        "        else:\n",
        "            return 'Temporal Features'\n",
        "\n",
        "    importance_df['category'] = importance_df['feature'].apply(get_feature_category)\n",
        "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "    # Calculate category importance\n",
        "    category_importance = importance_df.groupby('category')['importance'].sum().sort_values(ascending=False)\n",
        "\n",
        "    # Visualizations\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Top 20 features\n",
        "    plt.subplot(2, 1, 1)\n",
        "    top_features = importance_df.head(20)\n",
        "    sns.barplot(data=top_features, x='importance', y='feature')\n",
        "    plt.title('Top 20 Feature Importance (Gain)')\n",
        "\n",
        "    # Plot 2: Category importance\n",
        "    plt.subplot(2, 1, 2)\n",
        "    sns.barplot(x=category_importance.values, y=category_importance.index)\n",
        "    plt.title('Feature Category Importance')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed metrics\n",
        "    print(\"\\nDetailed Model Performance:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(importance_df[['feature', 'importance', 'category']].head(10))\n",
        "\n",
        "    return metrics, val_preds, importance_df, category_importance\n",
        "\n",
        "def rank_new_articles(model, df, feature_cols, n_articles=10):\n",
        "    \"\"\"Generate rankings for new articles.\"\"\"\n",
        "    sample_df = df.sample(n=n_articles)\n",
        "    predictions = model.predict(sample_df[feature_cols].values)\n",
        "\n",
        "    sample_df['predicted_score'] = predictions\n",
        "\n",
        "    # Calculate confidence scores\n",
        "    rank_pct = pd.Series(predictions).rank(pct=True, method='first')\n",
        "\n",
        "    # Define severity with confidence threshold\n",
        "    severity_thresholds = [0.2, 0.4, 0.6, 0.8]\n",
        "    severity_labels = ['Minor', 'Administrative', 'Moderate', 'Major', 'Critical']\n",
        "\n",
        "    sample_df['predicted_severity'] = pd.cut(\n",
        "        rank_pct,\n",
        "        bins=[0] + severity_thresholds + [1.0],\n",
        "        labels=severity_labels,\n",
        "        include_lowest=True\n",
        "    )\n",
        "\n",
        "    sample_df['confidence_score'] = abs(0.5 - abs(rank_pct - 0.5)) * 2\n",
        "\n",
        "    ranked_articles = sample_df.sort_values('predicted_score', ascending=False)\n",
        "\n",
        "    display_cols = [\n",
        "        'Title',\n",
        "        'severity_category',\n",
        "        'predicted_severity',\n",
        "        'confidence_score',\n",
        "        'reason_score',\n",
        "        'predicted_score',\n",
        "        'Impact_Factor',\n",
        "        'original_cited_by_posts_count'\n",
        "    ]\n",
        "\n",
        "    return ranked_articles[display_cols]\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    # Download required NLTK data\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "    try:\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_csv('your_data.csv')  # Replace with your file path\n",
        "\n",
        "    # Print class distribution\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(df['severity_category'].value_counts(normalize=True))\n",
        "\n",
        "    # Extract features\n",
        "    print(\"\\nExtracting features...\")"
      ],
      "metadata": {
        "id": "OMZN1DsWE4_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMTNhdQzJ-9J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}